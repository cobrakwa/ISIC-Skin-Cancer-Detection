{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "817f2848",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T17:07:15.560725Z",
     "iopub.status.busy": "2024-12-06T17:07:15.560380Z",
     "iopub.status.idle": "2024-12-06T17:07:57.627376Z",
     "shell.execute_reply": "2024-12-06T17:07:57.626409Z"
    },
    "papermill": {
     "duration": 42.073564,
     "end_time": "2024-12-06T17:07:57.629511",
     "exception": false,
     "start_time": "2024-12-06T17:07:15.555947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StratifiedGroupKFold\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomOverSampler\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munder_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomUnderSampler\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision.models import resnet50,resnet101\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from sklearn.metrics import roc_auc_score, classification_report,precision_recall_fscore_support,roc_curve, auc\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from typing import List, Dict, Optional, Generator\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import polars as pl\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import h5py\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855afbbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T17:07:57.635713Z",
     "iopub.status.busy": "2024-12-06T17:07:57.635446Z",
     "iopub.status.idle": "2024-12-06T17:07:57.652939Z",
     "shell.execute_reply": "2024-12-06T17:07:57.652199Z"
    },
    "papermill": {
     "duration": 0.022408,
     "end_time": "2024-12-06T17:07:57.654598",
     "exception": false,
     "start_time": "2024-12-06T17:07:57.632190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ISICModel(nn.Module):\n",
    "    def __init__(self, model_name, num_classes=1, drop_path_rate=0, drop_rate=0, pretrained=True, checkpoint_path=None):\n",
    "        super(ISICModel, self).__init__()\n",
    "        self.model = timm.create_model(\n",
    "            model_name, \n",
    "            pretrained=pretrained, \n",
    "            checkpoint_path=checkpoint_path,\n",
    "            drop_rate=drop_rate, \n",
    "            drop_path_rate=drop_path_rate)\n",
    "        \n",
    "        in_features = self.model.num_features\n",
    "        \n",
    "        self.model.head = nn.Sequential(\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, images):\n",
    "        return self.model(images)\n",
    "class FineTuneResNet101(nn.Module):\n",
    "    def __init__(self, num_classes, freeze_layers=6):\n",
    "        super().__init__()\n",
    "        self.model = resnet101(weights=None)\n",
    "        layers = list(self.model.named_children())\n",
    "        for name, child in layers[:freeze_layers]:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    " \n",
    "        num_ftrs = self.model.fc.in_features\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        print(self.get_layer_stats())\n",
    "        \n",
    "    def get_layer_stats(self):\n",
    "        \"\"\"Print trainable parameters per layer\"\"\"\n",
    "        for name, child in self.model.named_children():\n",
    "            trainable_params = sum(p.numel() for p in child.parameters() if p.requires_grad)\n",
    "            total_params = sum(p.numel() for p in child.parameters())\n",
    "            print(f\"{name}: {trainable_params}/{total_params} trainable parameters\")\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "class FineTuneResNet(nn.Module):\n",
    "    def __init__(self, num_classes, freeze_layers=6):\n",
    "        super().__init__()\n",
    "        self.model = resnet50(weights=None)\n",
    "        layers = list(self.model.named_children())\n",
    "        for name, child in layers[:freeze_layers]:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "        for param in self.model.layer4.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.model.layer3.parameters():\n",
    "            param.requires_grad = True\n",
    "        num_ftrs = self.model.fc.in_features\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  \n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        print(self.get_layer_stats())\n",
    "        \n",
    "    def get_layer_stats(self):\n",
    "        \"\"\"Print trainable parameters per layer\"\"\"\n",
    "        for name, child in self.model.named_children():\n",
    "            trainable_params = sum(p.numel() for p in child.parameters() if p.requires_grad)\n",
    "            total_params = sum(p.numel() for p in child.parameters())\n",
    "            print(f\"{name}: {trainable_params}/{total_params} trainable parameters\")\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "from torchvision.models import swin_t\n",
    "\n",
    "class FineTuneSwin(nn.Module):\n",
    "    def __init__(self, num_classes, freeze_layers=6):\n",
    "        super().__init__()\n",
    "        self.model = swin_t(weights=None)\n",
    "        \n",
    "        layers = list(self.model.named_children())\n",
    "        for name, child in layers[:freeze_layers]:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        num_ftrs = self.model.head.in_features\n",
    "    \n",
    "        self.model.head = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        print(self.get_layer_stats())\n",
    "        \n",
    "    def get_layer_stats(self):\n",
    "        for name, child in self.model.named_children():\n",
    "            trainable_params = sum(p.numel() for p in child.parameters() if p.requires_grad)\n",
    "            total_params = sum(p.numel() for p in child.parameters())\n",
    "            print(f\"{name}: {trainable_params}/{total_params} trainable parameters\")\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3965b651",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T17:07:57.660904Z",
     "iopub.status.busy": "2024-12-06T17:07:57.660608Z",
     "iopub.status.idle": "2024-12-06T17:07:57.691633Z",
     "shell.execute_reply": "2024-12-06T17:07:57.690997Z"
    },
    "papermill": {
     "duration": 0.03634,
     "end_time": "2024-12-06T17:07:57.693351",
     "exception": false,
     "start_time": "2024-12-06T17:07:57.657011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_data(path, num_cols, cat_cols):\n",
    "    err = 1e-8\n",
    "    id_col = 'isic_id'\n",
    "    return (\n",
    "        pl.read_csv(path)\n",
    "        .with_columns(\n",
    "            pl.col('age_approx').cast(pl.String).replace('NA', np.nan).cast(pl.Float64),\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.col(pl.Float64).fill_nan(pl.col(pl.Float64).median()),\n",
    "        )\n",
    "\n",
    "        .with_columns(\n",
    "            pl.col(cat_cols).cast(pl.Categorical),\n",
    "        )\n",
    "        .to_pandas()\n",
    "        .set_index(id_col)\n",
    "    )\n",
    "\n",
    "def preprocess(df_train, df_test, feature_cols, cat_cols):\n",
    "    \"\"\"\n",
    "    Preprocess the data by encoding categorical variables\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    feature_cols = feature_cols.copy()\n",
    "    encoder = OneHotEncoder(sparse_output=False, dtype=np.int32, handle_unknown='ignore')\n",
    "    encoder.fit(df_train[cat_cols])\n",
    "    new_cat_cols = [f'onehot_{i}' for i in range(len(encoder.get_feature_names_out()))]\n",
    "\n",
    "    df_train_encoded = encoder.transform(df_train[cat_cols])\n",
    "    df_test_encoded = encoder.transform(df_test[cat_cols])\n",
    "    \n",
    "    for i, col in enumerate(new_cat_cols):\n",
    "        df_train[col] = pd.Categorical(df_train_encoded[:, i])\n",
    "        df_test[col] = pd.Categorical(df_test_encoded[:, i])\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        if col in feature_cols:\n",
    "            feature_cols.remove(col)\n",
    "    feature_cols.extend(new_cat_cols)\n",
    "    \n",
    "    return df_train, df_test, feature_cols, new_cat_cols\n",
    "\n",
    "\n",
    "xgb_params = {\n",
    "    'enable_categorical': True,\n",
    "    'tree_method': 'hist',\n",
    "    'random_state': 42,\n",
    "    'learning_rate': 0.08501257473292347, \n",
    "    'lambda': 8.879624125465703, \n",
    "    'alpha': 0.6779926606782505, \n",
    "    'max_depth': 6, \n",
    "    'subsample': 0.6012681388711075, \n",
    "    'colsample_bytree': 0.8437772277074493, \n",
    "    'colsample_bylevel': 0.5476090898823716, \n",
    "    'colsample_bynode': 0.9928601203635129, \n",
    "    'scale_pos_weight': 3.29440313334688,\n",
    "    \"device\": \"cuda\"\n",
    "}\n",
    "\n",
    "def get_image_predictions(models, dataloader, device, model_names, save_dir=None):\n",
    "    \"\"\"Get predictions from multiple image models and save individually\"\"\"\n",
    "    \n",
    "    if save_dir:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "    all_predictions = []\n",
    "    for model, model_name in zip(models, model_names):\n",
    "        filename = f'{save_dir}/{model_name}_predictions.npy'\n",
    "        if os.path.exists(filename) and save_dir:\n",
    "            predictions = np.load(filename)\n",
    "            print(f\"Loaded existing predictions for {model_name} from {filename}\")\n",
    "        else:\n",
    "            model.eval()\n",
    "            predictions = []\n",
    "            with torch.no_grad():\n",
    "                for images in tqdm(dataloader, desc=f'Getting predictions for {model_name}'):\n",
    "                    images = images.to(device)\n",
    "                    outputs = model(images)\n",
    "                    probs = outputs.softmax(dim=1)[:, 1].cpu().numpy()\n",
    "                    predictions.extend(probs)\n",
    "            predictions = np.array(predictions)\n",
    "            if save_dir:\n",
    "                np.save(filename, predictions)\n",
    "                print(f\"Saved predictions for {model_name} to {filename}\")\n",
    "        \n",
    "        all_predictions.append(predictions)\n",
    "    \n",
    "    return np.stack(all_predictions, axis=1) \n",
    "\n",
    "def prepare_feature_matrix(df, image_preds,feature_cols):\n",
    "    \"\"\"Combine metadata features with image predictions\"\"\"\n",
    "    image_pred_df = pd.DataFrame(\n",
    "        image_preds, \n",
    "        columns=[f'model_pred_{i}' for i in range(image_preds.shape[1])]\n",
    "    )\n",
    "    \n",
    "    feature_cols = feature_cols + [f'model_pred_{i}' for i in range(image_preds.shape[1])]\n",
    "    df = pd.concat([df, image_pred_df], axis=1)\n",
    "    return df\n",
    "def run_model_old_xgb(df_train, df_test, xgb_params, feature_cols, reduce=True, columns_to_drop=None, save_dir='xgboost_models'):\n",
    "    \"\"\"\n",
    "    Train XGBoost models with cross-validation and optionally save them\n",
    "    \n",
    "    Args:\n",
    "        df_train: Training DataFrame\n",
    "        df_test: Test DataFrame\n",
    "        xgb_params: XGBoost parameters\n",
    "        feature_cols: List of feature columns\n",
    "        reduce: Whether to return mean metric\n",
    "        columns_to_drop: Columns to exclude from training\n",
    "        save_dir: Directory to save models (None to skip saving)\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import joblib\n",
    "    \n",
    "    columns_to_drop = [] if columns_to_drop is None else columns_to_drop\n",
    "    metric_list = []\n",
    "    models = []\n",
    "    group_col = 'patient_id'\n",
    "    \n",
    "    if save_dir:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    for random_seed in range(1):\n",
    "        random_seed = random_seed * 10 + 88\n",
    "        tsp = StratifiedGroupKFold(5, shuffle=True, random_state=random_seed)\n",
    "        \n",
    "        for fold_n, (train_index, val_index) in tqdm(enumerate(tsp.split(df_train, y=df_train.target, groups=df_train[group_col]))):\n",
    "            train_slice_x = df_train.iloc[train_index][[i for i in feature_cols if i not in columns_to_drop]]\n",
    "            val_slice_x = df_train.iloc[val_index][[i for i in feature_cols if i not in columns_to_drop]]\n",
    "            \n",
    "            train_slice_y = df_train.iloc[train_index]['target']\n",
    "            val_slice_y = df_train.iloc[val_index]['target']\n",
    "            \n",
    "            for col in [c for c in train_slice_x.columns if c.startswith('model_pred_')]:\n",
    "                train_slice_x[col] = train_slice_x[col] + np.random.normal(loc=0, scale=0.1, size=train_slice_x.shape[0])\n",
    "            \n",
    "            xgb_model = Pipeline([\n",
    "                ('sampler_1', RandomOverSampler(sampling_strategy=0.003, random_state=random_seed)),\n",
    "                ('sampler_2', RandomUnderSampler(sampling_strategy=0.01, random_state=random_seed)),\n",
    "                ('classifier', xgb.XGBClassifier(**xgb_params)),\n",
    "            ])\n",
    "            \n",
    "            xgb_model.fit(train_slice_x, train_slice_y)\n",
    "            preds = xgb_model.predict_proba(val_slice_x)[:, 1]\n",
    "            metric = custom_metric_raw(preds, val_slice_y.values)\n",
    "            metric_list.append(metric)\n",
    "            models.append(xgb_model)\n",
    "            \n",
    "            if save_dir:\n",
    "                model_path = os.path.join(save_dir, f'xgb_model_seed{random_seed}_fold{fold_n}.joblib')\n",
    "                joblib.dump(xgb_model, model_path)\n",
    "                print(f\"Saved model to {model_path}\")\n",
    "    \n",
    "    if reduce:\n",
    "        return np.mean(metric_list), models\n",
    "    else:\n",
    "        return metric_list, models\n",
    "def load_model(model_class,path , device='cuda'):\n",
    "    \"\"\"Load multiple PyTorch models from .pth files\"\"\"\n",
    "    \n",
    "    if model_class == 'FineTuneResNet101':\n",
    "        model = FineTuneResNet101(num_classes = 2)\n",
    "    elif model_class == 'FineTuneSwin':\n",
    "        model = FineTuneSwin(num_classes = 2)\n",
    "    elif model_class == 'eva_silu':\n",
    "        model = ISICModel(\n",
    "    model_name='eva02_small_patch14_224',\n",
    "    num_classes=2,\n",
    "    pretrained=False\n",
    ")\n",
    "    else:\n",
    "        print(f'invalid model class {model_class}')\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "class CombinedDataset(Dataset):\n",
    "    \"\"\"Dataset for reading images from either HDF5 or directory\"\"\"\n",
    "    def __init__(self, df, transform=None, hdf5_path=None, image_dir=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: DataFrame with image IDs\n",
    "            transform: Albumentations transforms\n",
    "            hdf5_path: Path to HDF5 file (if using HDF5)\n",
    "            image_dir: Path to image directory (if using individual files)\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.hdf5_path = hdf5_path\n",
    "        self.image_dir = image_dir\n",
    "        \n",
    "        if hdf5_path and image_dir:\n",
    "            raise ValueError(\"Specify either hdf5_path or image_dir, not both\")\n",
    "        if not (hdf5_path or image_dir):\n",
    "            raise ValueError(\"Must specify either hdf5_path or image_dir\")\n",
    "            \n",
    "        if hdf5_path:\n",
    "            with h5py.File(hdf5_path, 'r') as f:\n",
    "                self.available_ids = set(f.keys())\n",
    "                missing_ids = set(df['isic_id']) - self.available_ids\n",
    "                if missing_ids:\n",
    "                    raise ValueError(f\"Missing {len(missing_ids)} images in HDF5 file\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def _load_hdf5_image(self, image_id):\n",
    "        \"\"\"Load single image from HDF5 file\"\"\"\n",
    "        with h5py.File(self.hdf5_path, 'r') as f:\n",
    "            binary_data = f[image_id][()]\n",
    "            if isinstance(binary_data, str):\n",
    "                binary_data = binary_data.encode()\n",
    "            nparr = np.frombuffer(binary_data, np.uint8)\n",
    "            image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "            return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    def _load_file_image(self, image_id):\n",
    "        \"\"\"Load single image from file system\"\"\"\n",
    "        image_path = os.path.join(self.image_dir, f\"{image_id}.jpg\")\n",
    "        if not os.path.exists(image_path):\n",
    "            raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "        image = cv2.imread(image_path)\n",
    "        return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.df.iloc[idx]['isic_id']\n",
    "    \n",
    "        if self.hdf5_path:\n",
    "            image = self._load_hdf5_image(image_id)\n",
    "        else:\n",
    "            image = self._load_file_image(image_id)\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)['image']\n",
    "            \n",
    "        return image\n",
    "\n",
    "def create_dataloaders(df, batch_size=32, hdf5_path=None, image_dir=None, num_workers=4):\n",
    "    \"\"\"\n",
    "    Create dataloaders for either HDF5 or directory images\n",
    "    \"\"\"\n",
    "    transform = A.Compose([\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    " \n",
    "    dataset = CombinedDataset(\n",
    "        df=df,\n",
    "        transform=transform,\n",
    "        hdf5_path=hdf5_path,\n",
    "        image_dir=image_dir\n",
    "    )\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return dataloader\n",
    "def custom_metric_raw(y_hat, y_true):\n",
    "    min_tpr = 0.80\n",
    "    max_fpr = abs(1 - min_tpr)\n",
    "    \n",
    "    v_gt = abs(y_true - 1)\n",
    "    v_pred = np.array([1.0 - x for x in y_hat])\n",
    "    \n",
    "    partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "    partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
    "    \n",
    "    return partial_auc\n",
    "\n",
    "def get_model_predictions(models_list, df_test, feature_cols, columns_to_drop=None):\n",
    "    columns_to_drop = [] if not columns_to_drop else columns_to_drop\n",
    "    df_test_size = int(df_test.shape[0] // 2)\n",
    "    predictions_tmp = None\n",
    "    for model in models_list:\n",
    "        preds_tmp = model.predict_proba(\n",
    "                df_test[[i for i in feature_cols if i not in columns_to_drop]])[:, 1]\n",
    "\n",
    "        preds_tmp = pd.DataFrame({\"preds\": preds_tmp})\n",
    "        preds_tmp = preds_tmp['preds'].rank(pct=True)\n",
    "\n",
    "        if predictions_tmp is None:\n",
    "            predictions_tmp = preds_tmp.values\n",
    "        else:\n",
    "            predictions_tmp += preds_tmp.values\n",
    "\n",
    "    predictions_tmp = predictions_tmp / len(models_list)\n",
    "    return predictions_tmp\n",
    "def custom_metric(estimator, X, y_true):\n",
    "    y_hat = estimator.predict_proba(X)[:, 1]\n",
    "    partial_auc = custom_metric_raw(y_hat, y_true)\n",
    "    return partial_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e57c48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T17:07:57.699479Z",
     "iopub.status.busy": "2024-12-06T17:07:57.699203Z",
     "iopub.status.idle": "2024-12-06T17:08:11.139840Z",
     "shell.execute_reply": "2024-12-06T17:08:11.138733Z"
    },
    "papermill": {
     "duration": 13.445892,
     "end_time": "2024-12-06T17:08:11.141817",
     "exception": false,
     "start_time": "2024-12-06T17:07:57.695925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1: 0/9408 trainable parameters\n",
      "bn1: 0/128 trainable parameters\n",
      "relu: 0/0 trainable parameters\n",
      "maxpool: 0/0 trainable parameters\n",
      "layer1: 0/215808 trainable parameters\n",
      "layer2: 0/1219584 trainable parameters\n",
      "layer3: 26090496/26090496 trainable parameters\n",
      "layer4: 14964736/14964736 trainable parameters\n",
      "avgpool: 0/0 trainable parameters\n",
      "fc: 1182466/1182466 trainable parameters\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23/189821309.py:212: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: 0/27517818 trainable parameters\n",
      "norm: 0/1536 trainable parameters\n",
      "permute: 0/0 trainable parameters\n",
      "avgpool: 0/0 trainable parameters\n",
      "flatten: 0/0 trainable parameters\n",
      "head: 527106/527106 trainable parameters\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting predictions for FineTuneResNet101:   0%|          | 0/1 [00:00<?, ?it/s]/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: Using fork() can cause Polars to deadlock in the child process.\n",
      "In addition, using fork() with Python in general is a recipe for mysterious\n",
      "deadlocks and crashes.\n",
      "\n",
      "The most likely reason you are seeing this error is because you are using the\n",
      "multiprocessing module on Linux, which uses fork() by default. This will be\n",
      "fixed in Python 3.14. Until then, you want to use the \"spawn\" context instead.\n",
      "\n",
      "See https://docs.pola.rs/user-guide/misc/multiprocessing/ for details.\n",
      "\n",
      "  self.pid = os.fork()\n",
      "Getting predictions for FineTuneResNet101: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s]/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: Using fork() can cause Polars to deadlock in the child process.\n",
      "In addition, using fork() with Python in general is a recipe for mysterious\n",
      "deadlocks and crashes.\n",
      "\n",
      "The most likely reason you are seeing this error is because you are using the\n",
      "multiprocessing module on Linux, which uses fork() by default. This will be\n",
      "fixed in Python 3.14. Until then, you want to use the \"spawn\" context instead.\n",
      "\n",
      "See https://docs.pola.rs/user-guide/misc/multiprocessing/ for details.\n",
      "\n",
      "  self.pid = os.fork()\n",
      "Getting predictions for FineTuneResNet101: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it]\n",
      "Getting predictions for FineTuneSwin: 100%|██████████| 1/1 [00:00<00:00,  1.94it/s]\n",
      "Getting predictions for eva_silu: 100%|██████████| 1/1 [00:00<00:00,  4.00it/s]\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [17:08:10] WARNING: /workspace/src/common/error_msg.h:80: If you are loading a serialized model (like pickle in Python, RDS in R) or\n",
      "configuration generated by an older version of XGBoost, please export the model by calling\n",
      "`Booster.save_model` from that version first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html\n",
      "\n",
      "for more details about differences between saving model and serializing.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['isic_id', 'patient_id', 'age_approx', 'sex', 'anatom_site_general',\n",
      "       'clin_size_long_diam_mm', 'image_type', 'tbp_tile_type', 'tbp_lv_A',\n",
      "       'tbp_lv_Aext', 'tbp_lv_B', 'tbp_lv_Bext', 'tbp_lv_C', 'tbp_lv_Cext',\n",
      "       'tbp_lv_H', 'tbp_lv_Hext', 'tbp_lv_L', 'tbp_lv_Lext', 'tbp_lv_areaMM2',\n",
      "       'tbp_lv_area_perim_ratio', 'tbp_lv_color_std_mean', 'tbp_lv_deltaA',\n",
      "       'tbp_lv_deltaB', 'tbp_lv_deltaL', 'tbp_lv_deltaLB',\n",
      "       'tbp_lv_deltaLBnorm', 'tbp_lv_eccentricity', 'tbp_lv_location',\n",
      "       'tbp_lv_location_simple', 'tbp_lv_minorAxisMM',\n",
      "       'tbp_lv_nevi_confidence', 'tbp_lv_norm_border', 'tbp_lv_norm_color',\n",
      "       'tbp_lv_perimeterMM', 'tbp_lv_radial_color_std_max', 'tbp_lv_stdL',\n",
      "       'tbp_lv_stdLExt', 'tbp_lv_symm_2axis', 'tbp_lv_symm_2axis_angle',\n",
      "       'tbp_lv_x', 'tbp_lv_y', 'tbp_lv_z', 'attribution', 'copyright_license',\n",
      "       'onehot_0', 'onehot_1', 'onehot_2', 'onehot_3', 'onehot_4', 'onehot_5',\n",
      "       'onehot_6', 'onehot_7', 'onehot_8', 'onehot_9', 'onehot_10',\n",
      "       'onehot_11', 'onehot_12', 'onehot_13', 'onehot_14', 'onehot_15',\n",
      "       'onehot_16', 'onehot_17', 'onehot_18', 'onehot_19', 'onehot_20',\n",
      "       'onehot_21', 'onehot_22', 'onehot_23', 'onehot_24', 'onehot_25',\n",
      "       'onehot_26', 'onehot_27', 'onehot_28', 'onehot_29', 'onehot_30',\n",
      "       'onehot_31', 'onehot_32', 'onehot_33', 'onehot_34', 'onehot_35',\n",
      "       'onehot_36', 'onehot_37', 'onehot_38', 'onehot_39', 'onehot_40',\n",
      "       'onehot_41', 'onehot_42', 'onehot_43', 'onehot_44', 'onehot_45',\n",
      "       'onehot_46', 'model_pred_0', 'model_pred_1', 'model_pred_2'],\n",
      "      dtype='object')\n",
      "Loaded model from /kaggle/input/xgb/pytorch/default/1/xgb_model_seed88_fold0_noimage.joblib\n",
      "Loaded model from /kaggle/input/xgb/pytorch/default/1/xgb_model_seed88_fold1_noimage.joblib\n",
      "Loaded model from /kaggle/input/xgb/pytorch/default/1/xgb_model_seed88_fold2_noimage.joblib\n",
      "Loaded model from /kaggle/input/xgb/pytorch/default/1/xgb_model_seed88_fold3_noimage.joblib\n",
      "Loaded model from /kaggle/input/xgb/pytorch/default/1/xgb_model_seed88_fold4_noimage.joblib\n",
      "        isic_id    target\n",
      "0  ISIC_0015657  0.933333\n",
      "1  ISIC_0015729  0.533333\n",
      "2  ISIC_0015740  0.533333\n",
      "Submission saved to submission.csv\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import joblib\n",
    "def main():\n",
    "    num_cols = [\n",
    "        'age_approx', 'clin_size_long_diam_mm', 'tbp_lv_A', 'tbp_lv_Aext', \n",
    "        'tbp_lv_B', 'tbp_lv_Bext', 'tbp_lv_C', 'tbp_lv_Cext', 'tbp_lv_H', \n",
    "        'tbp_lv_Hext', 'tbp_lv_L', 'tbp_lv_Lext', 'tbp_lv_areaMM2', \n",
    "        'tbp_lv_area_perim_ratio', 'tbp_lv_color_std_mean', 'tbp_lv_deltaA', \n",
    "        'tbp_lv_deltaB', 'tbp_lv_deltaL', 'tbp_lv_deltaLB', 'tbp_lv_deltaLBnorm',\n",
    "        'tbp_lv_eccentricity', 'tbp_lv_minorAxisMM', 'tbp_lv_nevi_confidence',\n",
    "        'tbp_lv_norm_border', 'tbp_lv_norm_color', 'tbp_lv_perimeterMM',\n",
    "        'tbp_lv_radial_color_std_max', 'tbp_lv_stdL', 'tbp_lv_stdLExt',\n",
    "        'tbp_lv_symm_2axis', 'tbp_lv_symm_2axis_angle', 'tbp_lv_x', 'tbp_lv_y',\n",
    "        'tbp_lv_z'\n",
    "    ]\n",
    "    cat_cols = ['sex', 'anatom_site_general', 'tbp_tile_type', 'tbp_lv_location', \n",
    "                'tbp_lv_location_simple', 'attribution']\n",
    "    \n",
    "    feature_cols = num_cols + cat_cols \n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_paths = {\n",
    "        'FineTuneResNet101': '/kaggle/input/resnet101_model.pth/pytorch/default/1/resnet101_model.pth',\n",
    "        'FineTuneSwin': '/kaggle/input/swin/pytorch/default/1/swin_model.pth',\n",
    "        'eva_silu': '/kaggle/input/evaa/pytorch/default/1/eva_silu_model.pth'\n",
    "    }\n",
    "    model_names = [name for name, _ in model_paths.items()]\n",
    "    models = [load_model(modelclass, path).to(device) for modelclass, path in model_paths.items()]\n",
    "\n",
    "\n",
    "    test_path = '/kaggle/input/isic-2024-challenge/test-metadata.csv'\n",
    "    df_test = read_data(test_path, num_cols, cat_cols).reset_index()\n",
    "    train_path = '/kaggle/input/isic-2024-challenge/train-metadata.csv'\n",
    "    df_train = read_data(train_path, num_cols, cat_cols).reset_index()\n",
    "    df_train, df_test, updated_feature_cols, new_cat_cols = preprocess(\n",
    "        df_train, df_test, num_cols, cat_cols\n",
    "    )\n",
    "    feature_cols = updated_feature_cols\n",
    "    # print(feature_cols)\n",
    "\n",
    "    test_dataloader = create_dataloaders(\n",
    "        df=df_test,\n",
    "        hdf5_path='/kaggle/input/isic-2024-challenge/test-image.hdf5',\n",
    "        batch_size=32\n",
    "    )\n",
    "    if models:\n",
    "        test_preds = get_image_predictions(\n",
    "            models, test_dataloader, device, model_names\n",
    "        )\n",
    "        df_test = prepare_feature_matrix(df_test, test_preds, feature_cols)\n",
    "    print(df_test.columns)\n",
    "    models = []\n",
    "    xgb_model_dir = '/kaggle/input/xgb/pytorch/default/1' \n",
    "    for model_path in sorted(glob.glob(os.path.join(xgb_model_dir, '*.joblib'))):\n",
    "        model = joblib.load(model_path)\n",
    "        models.append(model)\n",
    "        print(f\"Loaded model from {model_path}\")\n",
    "    df_test = df_test.drop(columns=['patient_id'])\n",
    "\n",
    "    feature_cols += ['model_pred_0', 'model_pred_1','model_pred_2']\n",
    "    predictions = get_model_predictions(models, df_test, feature_cols)\n",
    "    \n",
    "    df_test['target'] = predictions\n",
    "    df_test[['isic_id', 'target']].to_csv('submission.csv', index=False)\n",
    "    print(df_test[['isic_id', 'target']])\n",
    "    print(\"Submission saved to submission.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8336bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-06T17:06:25.329751Z",
     "iopub.status.busy": "2024-12-06T17:06:25.328745Z",
     "iopub.status.idle": "2024-12-06T17:06:25.369348Z",
     "shell.execute_reply": "2024-12-06T17:06:25.368020Z",
     "shell.execute_reply.started": "2024-12-06T17:06:25.329713Z"
    },
    "papermill": {
     "duration": 0.00302,
     "end_time": "2024-12-06T17:08:11.148597",
     "exception": false,
     "start_time": "2024-12-06T17:08:11.145577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7dbea0",
   "metadata": {
    "papermill": {
     "duration": 0.002935,
     "end_time": "2024-12-06T17:08:11.154653",
     "exception": false,
     "start_time": "2024-12-06T17:08:11.151718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9094797,
     "sourceId": 63056,
     "sourceType": "competition"
    },
    {
     "modelId": 183474,
     "modelInstanceId": 161088,
     "sourceId": 188941,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 183812,
     "modelInstanceId": 161445,
     "sourceId": 189353,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 184968,
     "modelInstanceId": 162620,
     "sourceId": 190779,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 184977,
     "modelInstanceId": 162629,
     "sourceId": 190788,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "CV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 60.700228,
   "end_time": "2024-12-06T17:08:13.776171",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-06T17:07:13.075943",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
