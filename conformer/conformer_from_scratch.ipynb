{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "data_dir = \"../data/balanced_data\"\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  \n",
    "])\n",
    "\n",
    "\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "labels = [label for _, label in dataset.samples]\n",
    "filenames = [os.path.basename(path) for path, _ in dataset.samples]\n",
    "\n",
    "\n",
    "label_1_indices = [i for i, label in enumerate(labels) if label == 1]\n",
    "label_0_indices = [i for i, label in enumerate(labels) if label == 0]\n",
    "\n",
    "\n",
    "original_label_1_indices = [i for i in label_1_indices if filenames[i].startswith(\"original\")]\n",
    "\n",
    "\n",
    "label_1_test_size = int(0.1 * len(label_1_indices))  \n",
    "label_1_test_indices = original_label_1_indices[:label_1_test_size]\n",
    "remaining_label_1_indices = list(set(label_1_indices) - set(label_1_test_indices))\n",
    "\n",
    "\n",
    "label_0_test_size = int(0.1 * len(label_0_indices))  \n",
    "label_0_test_indices = label_0_indices[:label_0_test_size]\n",
    "remaining_label_0_indices = label_0_indices[label_0_test_size:]\n",
    "\n",
    "\n",
    "label_1_train_indices, label_1_val_indices = train_test_split(\n",
    "    remaining_label_1_indices, test_size=0.2, stratify=[labels[i] for i in remaining_label_1_indices], random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "label_0_train_indices, label_0_val_indices = train_test_split(\n",
    "    remaining_label_0_indices, test_size=0.2, stratify=[labels[i] for i in remaining_label_0_indices], random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "train_indices = label_1_train_indices + label_0_train_indices\n",
    "val_indices = label_1_val_indices + label_0_val_indices\n",
    "test_indices = label_1_test_indices + label_0_test_indices\n",
    "\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "train_counts = Counter([labels[i] for i in train_indices])\n",
    "val_counts = Counter([labels[i] for i in val_indices])\n",
    "test_counts = Counter([labels[i] for i in test_indices])\n",
    "\n",
    "print(f\"Train class distribution: {train_counts}\")\n",
    "print(f\"Validation class distribution: {val_counts}\")\n",
    "print(f\"Test class distribution: {test_counts}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConformerTinyBinary(nn.Module):\n",
    "    def __init__(self, img_size=128, num_classes=5, embed_dim=32, num_heads=4, num_transformer_layers=1):\n",
    "        super(ConformerTinyBinary, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, embed_dim, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(embed_dim, embed_dim, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(embed_dim, embed_dim, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(embed_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads),\n",
    "            num_layers=num_transformer_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        b, c, h, w = x.shape\n",
    "        x = x.view(b, c, -1).permute(2, 0, 1)  \n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=0)  \n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, classification_report\n",
    "from scipy import interp\n",
    "\n",
    "model_dir = 'scratch_augmentless/saved_models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_model(epoch, model, optimizer, path, best=False):\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(state, path)\n",
    "    if best:\n",
    "        best_path = os.path.join(model_dir, f\"best_model_epoch{epoch}.pth\")\n",
    "        torch.save(state, best_path)\n",
    "\n",
    "def load_model(path, model, optimizer):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return checkpoint['epoch']\n",
    "\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    return running_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += torch.sum(preds == labels).item()\n",
    "    return running_loss / len(dataloader.dataset), correct / len(dataloader.dataset)\n",
    "\n",
    "def custom_metric(y_true, y_pred, min_tpr=0.8):\n",
    "    \"\"\"\n",
    "    Calculate the partial AUC (pAUC) based on a minimum TPR threshold.\n",
    "\n",
    "    Args:\n",
    "        y_true (array): True binary labels.\n",
    "        y_pred (array): Predicted probabilities.\n",
    "        min_tpr (float): Minimum TPR threshold (default: 0.8).\n",
    "\n",
    "    Returns:\n",
    "        float: Scaled pAUC value.\n",
    "    \"\"\"\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    max_fpr = 1 - min_tpr  \n",
    "    v_gt = abs(y_true - 1)  \n",
    "    v_pred = 1.0 - y_pred  \n",
    "\n",
    "    \n",
    "    pauc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "\n",
    "    \n",
    "    pauc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (pauc_scaled - 0.5)\n",
    "\n",
    "    return pauc\n",
    "\n",
    "\n",
    "\n",
    "def compute_auc_and_roc(model, dataloader, device, min_tpr=0.8):\n",
    "    \"\"\"\n",
    "    Computes the ROC, AUC, and partial AUC for the given model and dataloader.\n",
    "    \n",
    "    Args:\n",
    "        model: The PyTorch model to evaluate.\n",
    "        dataloader: DataLoader for evaluation data.\n",
    "        device: Device to run the computations on (CPU or GPU).\n",
    "        min_tpr: Minimum TPR threshold for partial AUC computation.\n",
    "    \n",
    "    Returns:\n",
    "        fpr: False positive rates.\n",
    "        tpr: True positive rates.\n",
    "        roc_auc: Full area under the ROC curve.\n",
    "        partial_auc: Partial area under the ROC curve for TPR >= min_tpr.\n",
    "        y_true: Ground truth labels.\n",
    "        y_scores: Predicted scores for the positive class.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            probabilities = torch.softmax(outputs, dim=1)[:, 1]\n",
    "            \n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_scores.extend(probabilities.cpu().numpy())\n",
    "    \n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    roc_auc = roc_auc_score(y_true, y_scores)  \n",
    "\n",
    "    \n",
    "    normalized_pauc = custom_metric(y_true, y_scores, min_tpr=min_tpr)\n",
    "\n",
    "    return fpr, tpr, roc_auc, normalized_pauc, y_true, y_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fine_tuned_model = ConformerTinyBinary(num_classes=2)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "fine_tuned_model = fine_tuned_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 1e-4  \n",
    "num_epochs = 20  \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(fine_tuned_model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "val_aucs = []\n",
    "val_paucs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_loss = train_one_epoch(fine_tuned_model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    \n",
    "    train_loss, train_acc = validate(fine_tuned_model, train_loader, criterion, device)\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    \n",
    "    val_loss, val_acc = validate(fine_tuned_model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    \n",
    "    fpr, tpr, roc_auc, partial_auc, _, _ = compute_auc_and_roc(fine_tuned_model, val_loader, device, min_tpr=0.8)\n",
    "\n",
    "    val_aucs.append(roc_auc)\n",
    "    val_paucs.append(partial_auc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val AUC: {roc_auc:.4f}, Val pAUC: {partial_auc:.4f}\")\n",
    "\n",
    "    \n",
    "    save_model(epoch, fine_tuned_model, optimizer, os.path.join(model_dir, f\"epoch{epoch+1}.pth\"))\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        save_model(epoch+1, fine_tuned_model, optimizer, os.path.join(model_dir, f\"best_model_epoch{epoch+1}.pth\"), best=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "model_files = glob.glob(os.path.join(model_dir, \"best_model_epoch*.pth\"))\n",
    "if not model_files:\n",
    "    raise FileNotFoundError(f\"No model files found in {model_dir} matching pattern 'best_model_epoch*.pth'\")\n",
    "\n",
    "\n",
    "epoch_numbers = [\n",
    "    int(re.search(r\"best_model_epoch(\\d+)\\.pth\", os.path.basename(file)).group(1))\n",
    "    for file in model_files\n",
    "]\n",
    "\n",
    "\n",
    "highest_epoch = max(epoch_numbers)\n",
    "print(f\"Highest epoch found: {highest_epoch}\")\n",
    "\n",
    "\n",
    "best_model_path = os.path.join(model_dir, f\"best_model_epoch{highest_epoch}.pth\")\n",
    "print(f\"Loading best model from: {best_model_path}\")\n",
    "checkpoint = torch.load(best_model_path)\n",
    "fine_tuned_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "\n",
    "fpr, tpr, roc_auc, partial_auc, y_true, y_scores = compute_auc_and_roc(fine_tuned_model, test_loader, device, min_tpr=0.8)\n",
    "\n",
    "\n",
    "y_pred = [1 if score >= 0.5 else 0 for score in y_scores]\n",
    "class_report = classification_report(y_true, y_pred, target_names=dataset.classes)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "\n",
    "report_path = os.path.join(model_dir, \"classification_report.txt\")\n",
    "with open(report_path, \"w\") as report_file:\n",
    "    report_file.write(class_report)\n",
    "\n",
    "print(f\"Classification report saved at: {report_path}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.4f}\", linewidth=2)\n",
    "plt.fill_between(fpr, tpr, alpha=0.2, label=f\"pAUC = {partial_auc:.4f} (FPR ≥ 0.8)\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve (Test Set)\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "roc_plot_path = os.path.join(model_dir, \"test_roc_curve.png\")\n",
    "plt.savefig(roc_plot_path, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Test ROC Curve plot saved at: {roc_plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss', marker='o')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Train vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.text(num_epochs - 1, val_losses[-1], f\"Final Val Loss: {val_losses[-1]:.4f}\", \n",
    "         fontsize=10, color='orange')\n",
    "\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(range(1, num_epochs + 1), train_accuracies, label='Train Accuracy', marker='o', color='blue')\n",
    "plt.plot(range(1, num_epochs + 1), val_accuracies, label='Validation Accuracy', marker='o', color='orange')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Train vs Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "\n",
    "test_accuracy = sum([y_true[i] == y_pred[i] for i in range(len(y_true))]) / len(y_true)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "plt.text(num_epochs - 1, val_accuracies[-1], f\"Test Accuracy: {test_accuracy:.4f}\", \n",
    "         fontsize=10, color='red', bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(range(1, num_epochs + 1), val_aucs, label=\"Validation AUC\", marker='o', color='green')\n",
    "plt.plot(range(1, num_epochs + 1), val_paucs, label=\"Validation pAUC (FPR ≤ 0.1)\", marker='o', color='purple')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.title(\"Validation AUC and pAUC\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.text(num_epochs - 1, val_aucs[-1], f\"Test AUC: {roc_auc:.4f}\", \n",
    "         fontsize=10, color='red', bbox=dict(facecolor='white', alpha=0.5))\n",
    "plt.text(num_epochs - 1, val_paucs[-1], f\"Test pAUC: {partial_auc:.4f}\", \n",
    "         fontsize=10, color='red', bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "\n",
    "combined_plot_path = os.path.join(model_dir, \"combined_metrics.png\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(combined_plot_path, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Combined metrics plot saved at: {combined_plot_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
