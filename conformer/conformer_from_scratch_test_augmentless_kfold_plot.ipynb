{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "data_dir = \"../data/balanced_data\"\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "labels = [label for _, label in dataset.samples]\n",
    "\n",
    "\n",
    "test_size = 0.1  \n",
    "remaining_indices, test_indices = train_test_split(\n",
    "    range(len(labels)),\n",
    "    test_size=test_size,\n",
    "    stratify=labels,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "remaining_labels = [labels[i] for i in remaining_indices]\n",
    "remaining_dataset = Subset(dataset, remaining_indices)\n",
    "\n",
    "\n",
    "k_folds = 5\n",
    "stratified_kfold = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=SEED)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConformerTinyBinary(nn.Module):\n",
    "    def __init__(self, img_size=128, num_classes=2, embed_dim=256, num_heads=8, num_transformer_layers=3, dropout=0.2):\n",
    "        super(ConformerTinyBinary, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, embed_dim, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(dropout),  \n",
    "\n",
    "            nn.Conv2d(embed_dim, embed_dim, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(dropout),  \n",
    "\n",
    "            nn.Conv2d(embed_dim, embed_dim, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(dropout)   \n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads),\n",
    "            num_layers=num_transformer_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        b, c, h, w = x.shape\n",
    "        x = x.view(b, c, -1).permute(2, 0, 1)  \n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=0)  \n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, classification_report\n",
    "from scipy import interp\n",
    "\n",
    "model_dir = 'scratch_test_augmentless_kfold/saved_models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_model(epoch, model, optimizer, path, best=False):\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(state, path)\n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "def load_model(path, model, optimizer):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return checkpoint['epoch']\n",
    "\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    return running_loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += torch.sum(preds == labels).item()\n",
    "    return running_loss / len(dataloader.dataset), correct / len(dataloader.dataset)\n",
    "\n",
    "def custom_metric(y_true, y_pred, min_tpr=0.8):\n",
    "    \"\"\"\n",
    "    Calculate the partial AUC (pAUC) based on a minimum TPR threshold.\n",
    "\n",
    "    Args:\n",
    "        y_true (array): True binary labels.\n",
    "        y_pred (array): Predicted probabilities.\n",
    "        min_tpr (float): Minimum TPR threshold (default: 0.8).\n",
    "\n",
    "    Returns:\n",
    "        float: Scaled pAUC value.\n",
    "    \"\"\"\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    max_fpr = 1 - min_tpr  \n",
    "    v_gt = abs(y_true - 1)  \n",
    "    v_pred = 1.0 - y_pred  \n",
    "\n",
    "    \n",
    "    pauc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "\n",
    "    \n",
    "    pauc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (pauc_scaled - 0.5)\n",
    "\n",
    "    return pauc\n",
    "\n",
    "\n",
    "\n",
    "def compute_auc_and_roc(model, dataloader, device, min_tpr=0.8):\n",
    "    \"\"\"\n",
    "    Computes the ROC, AUC, and partial AUC for the given model and dataloader.\n",
    "    \n",
    "    Args:\n",
    "        model: The PyTorch model to evaluate.\n",
    "        dataloader: DataLoader for evaluation data.\n",
    "        device: Device to run the computations on (CPU or GPU).\n",
    "        min_tpr: Minimum TPR threshold for partial AUC computation.\n",
    "    \n",
    "    Returns:\n",
    "        fpr: False positive rates.\n",
    "        tpr: True positive rates.\n",
    "        roc_auc: Full area under the ROC curve.\n",
    "        partial_auc: Partial area under the ROC curve for TPR >= min_tpr.\n",
    "        y_true: Ground truth labels.\n",
    "        y_scores: Predicted scores for the positive class.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            probabilities = torch.softmax(outputs, dim=1)[:, 1]\n",
    "            \n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_scores.extend(probabilities.cpu().numpy())\n",
    "    \n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    roc_auc = roc_auc_score(y_true, y_scores)  \n",
    "\n",
    "    \n",
    "    normalized_pauc = custom_metric(y_true, y_scores, min_tpr=min_tpr)\n",
    "\n",
    "    return fpr, tpr, roc_auc, normalized_pauc, y_true, y_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "batch_size = 64\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonec\\miniconda3\\envs\\general_ai\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\jonec\\miniconda3\\envs\\general_ai\\lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 1/50 - Train Loss: 0.7377, Train Acc: 0.4991, Val Loss: 0.8473, Val Acc: 0.5000\n",
      "Fold 1, Epoch 2/50 - Train Loss: 0.6769, Train Acc: 0.5097, Val Loss: 0.7109, Val Acc: 0.5141\n",
      "Fold 1, Epoch 3/50 - Train Loss: 0.6392, Train Acc: 0.5257, Val Loss: 0.7366, Val Acc: 0.5423\n",
      "Fold 1, Epoch 4/50 - Train Loss: 0.6246, Train Acc: 0.5717, Val Loss: 0.6611, Val Acc: 0.5986\n",
      "Fold 1, Epoch 5/50 - Train Loss: 0.6211, Train Acc: 0.5894, Val Loss: 0.6848, Val Acc: 0.6268\n",
      "Fold 1, Epoch 6/50 - Train Loss: 0.6222, Train Acc: 0.7310, Val Loss: 0.5177, Val Acc: 0.7183\n",
      "Fold 1, Epoch 7/50 - Train Loss: 0.5993, Train Acc: 0.7327, Val Loss: 0.4931, Val Acc: 0.7465\n",
      "Fold 1, Epoch 8/50 - Train Loss: 0.5562, Train Acc: 0.7469, Val Loss: 0.4841, Val Acc: 0.7887\n",
      "Fold 1, Epoch 9/50 - Train Loss: 0.5449, Train Acc: 0.7646, Val Loss: 0.5205, Val Acc: 0.7606\n",
      "Fold 1, Epoch 10/50 - Train Loss: 0.5285, Train Acc: 0.7575, Val Loss: 0.5458, Val Acc: 0.7676\n",
      "Fold 1, Epoch 11/50 - Train Loss: 0.5255, Train Acc: 0.7398, Val Loss: 0.5519, Val Acc: 0.7324\n",
      "Fold 1, Epoch 12/50 - Train Loss: 0.5081, Train Acc: 0.7717, Val Loss: 0.5284, Val Acc: 0.7817\n",
      "Fold 1, Epoch 13/50 - Train Loss: 0.4984, Train Acc: 0.7929, Val Loss: 0.5010, Val Acc: 0.7817\n",
      "Fold 1, Epoch 14/50 - Train Loss: 0.4809, Train Acc: 0.7664, Val Loss: 0.5634, Val Acc: 0.7746\n",
      "Fold 1, Epoch 15/50 - Train Loss: 0.4842, Train Acc: 0.8018, Val Loss: 0.5273, Val Acc: 0.8028\n",
      "Fold 1, Epoch 16/50 - Train Loss: 0.4538, Train Acc: 0.8142, Val Loss: 0.4859, Val Acc: 0.7958\n",
      "Fold 1, Epoch 17/50 - Train Loss: 0.4696, Train Acc: 0.7858, Val Loss: 0.5564, Val Acc: 0.7676\n",
      "Fold 1, Epoch 18/50 - Train Loss: 0.4685, Train Acc: 0.8035, Val Loss: 0.5560, Val Acc: 0.7958\n",
      "Fold 1, Epoch 19/50 - Train Loss: 0.4708, Train Acc: 0.8372, Val Loss: 0.4658, Val Acc: 0.8028\n",
      "Fold 1, Epoch 20/50 - Train Loss: 0.4389, Train Acc: 0.7381, Val Loss: 0.6285, Val Acc: 0.6901\n",
      "Fold 1, Epoch 21/50 - Train Loss: 0.4529, Train Acc: 0.8336, Val Loss: 0.4594, Val Acc: 0.7817\n",
      "Fold 1, Epoch 22/50 - Train Loss: 0.4527, Train Acc: 0.8354, Val Loss: 0.4461, Val Acc: 0.8028\n",
      "Fold 1, Epoch 23/50 - Train Loss: 0.4089, Train Acc: 0.8124, Val Loss: 0.5857, Val Acc: 0.7817\n",
      "Fold 1, Epoch 24/50 - Train Loss: 0.4486, Train Acc: 0.8248, Val Loss: 0.5369, Val Acc: 0.7676\n",
      "Fold 1, Epoch 25/50 - Train Loss: 0.4557, Train Acc: 0.8513, Val Loss: 0.4297, Val Acc: 0.7958\n",
      "Fold 1, Epoch 26/50 - Train Loss: 0.4204, Train Acc: 0.8407, Val Loss: 0.5109, Val Acc: 0.7746\n",
      "Fold 1, Epoch 27/50 - Train Loss: 0.3956, Train Acc: 0.8549, Val Loss: 0.4901, Val Acc: 0.7817\n",
      "Fold 1, Epoch 28/50 - Train Loss: 0.4161, Train Acc: 0.8212, Val Loss: 0.5513, Val Acc: 0.7535\n",
      "Fold 1, Epoch 29/50 - Train Loss: 0.4052, Train Acc: 0.8425, Val Loss: 0.5584, Val Acc: 0.7676\n",
      "Fold 1, Epoch 30/50 - Train Loss: 0.3836, Train Acc: 0.8655, Val Loss: 0.4876, Val Acc: 0.8028\n",
      "Fold 1, Epoch 31/50 - Train Loss: 0.4152, Train Acc: 0.8796, Val Loss: 0.4507, Val Acc: 0.8099\n",
      "Fold 1, Epoch 32/50 - Train Loss: 0.3787, Train Acc: 0.8195, Val Loss: 0.6806, Val Acc: 0.7394\n",
      "Fold 1, Epoch 33/50 - Train Loss: 0.4107, Train Acc: 0.8708, Val Loss: 0.4497, Val Acc: 0.8239\n",
      "Fold 1, Epoch 34/50 - Train Loss: 0.3734, Train Acc: 0.8566, Val Loss: 0.5456, Val Acc: 0.7817\n",
      "Fold 1, Epoch 35/50 - Train Loss: 0.3679, Train Acc: 0.8637, Val Loss: 0.5337, Val Acc: 0.7746\n",
      "Fold 1, Epoch 36/50 - Train Loss: 0.4147, Train Acc: 0.8850, Val Loss: 0.4716, Val Acc: 0.8310\n",
      "Fold 1, Epoch 37/50 - Train Loss: 0.3770, Train Acc: 0.8779, Val Loss: 0.4657, Val Acc: 0.8099\n",
      "Fold 1, Epoch 38/50 - Train Loss: 0.3852, Train Acc: 0.8761, Val Loss: 0.5382, Val Acc: 0.7746\n",
      "Fold 1, Epoch 39/50 - Train Loss: 0.3216, Train Acc: 0.8814, Val Loss: 0.4837, Val Acc: 0.8169\n",
      "Fold 1, Epoch 40/50 - Train Loss: 0.3597, Train Acc: 0.9009, Val Loss: 0.4881, Val Acc: 0.7746\n",
      "Fold 1, Epoch 41/50 - Train Loss: 0.3383, Train Acc: 0.8708, Val Loss: 0.5780, Val Acc: 0.7606\n",
      "Fold 1, Epoch 42/50 - Train Loss: 0.3545, Train Acc: 0.8832, Val Loss: 0.5150, Val Acc: 0.7958\n",
      "Fold 1, Epoch 43/50 - Train Loss: 0.3588, Train Acc: 0.8531, Val Loss: 0.6463, Val Acc: 0.7606\n",
      "Fold 1, Epoch 44/50 - Train Loss: 0.3038, Train Acc: 0.9080, Val Loss: 0.5307, Val Acc: 0.7958\n",
      "Fold 1, Epoch 45/50 - Train Loss: 0.3127, Train Acc: 0.8991, Val Loss: 0.5847, Val Acc: 0.7887\n",
      "Fold 1, Epoch 46/50 - Train Loss: 0.2854, Train Acc: 0.8761, Val Loss: 0.6854, Val Acc: 0.7817\n",
      "Fold 1, Epoch 47/50 - Train Loss: 0.2893, Train Acc: 0.9115, Val Loss: 0.6025, Val Acc: 0.7887\n",
      "Fold 1, Epoch 48/50 - Train Loss: 0.2725, Train Acc: 0.9133, Val Loss: 0.6608, Val Acc: 0.7746\n",
      "Fold 1, Epoch 49/50 - Train Loss: 0.2719, Train Acc: 0.8637, Val Loss: 0.7615, Val Acc: 0.7465\n",
      "Fold 1, Epoch 50/50 - Train Loss: 0.2917, Train Acc: 0.9044, Val Loss: 0.7078, Val Acc: 0.7676\n",
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonec\\AppData\\Local\\Temp\\ipykernel_23712\\890437736.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path)['model_state_dict'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2, Epoch 1/50 - Train Loss: 0.7511, Train Acc: 0.5434, Val Loss: 0.7753, Val Acc: 0.5000\n",
      "Fold 2, Epoch 2/50 - Train Loss: 0.6795, Train Acc: 0.5469, Val Loss: 0.6736, Val Acc: 0.5775\n",
      "Fold 2, Epoch 3/50 - Train Loss: 0.6464, Train Acc: 0.6301, Val Loss: 0.7019, Val Acc: 0.5563\n",
      "Fold 2, Epoch 4/50 - Train Loss: 0.6062, Train Acc: 0.6867, Val Loss: 0.6156, Val Acc: 0.6690\n",
      "Fold 2, Epoch 5/50 - Train Loss: 0.5953, Train Acc: 0.7080, Val Loss: 0.6267, Val Acc: 0.6972\n",
      "Fold 2, Epoch 6/50 - Train Loss: 0.5716, Train Acc: 0.7274, Val Loss: 0.5636, Val Acc: 0.7394\n",
      "Fold 2, Epoch 7/50 - Train Loss: 0.5693, Train Acc: 0.7398, Val Loss: 0.5702, Val Acc: 0.6972\n",
      "Fold 2, Epoch 8/50 - Train Loss: 0.5434, Train Acc: 0.7823, Val Loss: 0.5307, Val Acc: 0.7535\n",
      "Fold 2, Epoch 9/50 - Train Loss: 0.5439, Train Acc: 0.7522, Val Loss: 0.5401, Val Acc: 0.7183\n",
      "Fold 2, Epoch 10/50 - Train Loss: 0.5059, Train Acc: 0.7770, Val Loss: 0.5416, Val Acc: 0.7394\n",
      "Fold 2, Epoch 11/50 - Train Loss: 0.5032, Train Acc: 0.7717, Val Loss: 0.5176, Val Acc: 0.7606\n",
      "Fold 2, Epoch 12/50 - Train Loss: 0.5028, Train Acc: 0.7894, Val Loss: 0.5036, Val Acc: 0.7465\n",
      "Fold 2, Epoch 13/50 - Train Loss: 0.5014, Train Acc: 0.7823, Val Loss: 0.5040, Val Acc: 0.7606\n",
      "Fold 2, Epoch 14/50 - Train Loss: 0.4875, Train Acc: 0.8071, Val Loss: 0.4717, Val Acc: 0.7676\n",
      "Fold 2, Epoch 15/50 - Train Loss: 0.5048, Train Acc: 0.7717, Val Loss: 0.5136, Val Acc: 0.7394\n",
      "Fold 2, Epoch 16/50 - Train Loss: 0.4995, Train Acc: 0.8053, Val Loss: 0.4889, Val Acc: 0.7254\n",
      "Fold 2, Epoch 17/50 - Train Loss: 0.4627, Train Acc: 0.8177, Val Loss: 0.4644, Val Acc: 0.7887\n",
      "Fold 2, Epoch 18/50 - Train Loss: 0.4588, Train Acc: 0.7717, Val Loss: 0.5035, Val Acc: 0.7606\n",
      "Fold 2, Epoch 19/50 - Train Loss: 0.4597, Train Acc: 0.8372, Val Loss: 0.4702, Val Acc: 0.7676\n",
      "Fold 2, Epoch 20/50 - Train Loss: 0.4451, Train Acc: 0.7752, Val Loss: 0.5001, Val Acc: 0.7535\n",
      "Fold 2, Epoch 21/50 - Train Loss: 0.4300, Train Acc: 0.8389, Val Loss: 0.5274, Val Acc: 0.7676\n",
      "Fold 2, Epoch 22/50 - Train Loss: 0.4456, Train Acc: 0.7823, Val Loss: 0.4914, Val Acc: 0.7746\n",
      "Fold 2, Epoch 23/50 - Train Loss: 0.4185, Train Acc: 0.8407, Val Loss: 0.4619, Val Acc: 0.7817\n",
      "Fold 2, Epoch 24/50 - Train Loss: 0.4367, Train Acc: 0.8637, Val Loss: 0.4278, Val Acc: 0.8169\n",
      "Fold 2, Epoch 25/50 - Train Loss: 0.4100, Train Acc: 0.8602, Val Loss: 0.4465, Val Acc: 0.8239\n",
      "Fold 2, Epoch 26/50 - Train Loss: 0.3943, Train Acc: 0.8513, Val Loss: 0.4784, Val Acc: 0.7958\n",
      "Fold 2, Epoch 27/50 - Train Loss: 0.4187, Train Acc: 0.8336, Val Loss: 0.4601, Val Acc: 0.7958\n",
      "Fold 2, Epoch 28/50 - Train Loss: 0.3884, Train Acc: 0.8496, Val Loss: 0.5045, Val Acc: 0.7746\n",
      "Fold 2, Epoch 29/50 - Train Loss: 0.3749, Train Acc: 0.8796, Val Loss: 0.4772, Val Acc: 0.8099\n",
      "Fold 2, Epoch 30/50 - Train Loss: 0.3644, Train Acc: 0.8301, Val Loss: 0.5201, Val Acc: 0.7676\n",
      "Fold 2, Epoch 31/50 - Train Loss: 0.4007, Train Acc: 0.8779, Val Loss: 0.4522, Val Acc: 0.8239\n",
      "Fold 2, Epoch 32/50 - Train Loss: 0.3977, Train Acc: 0.8584, Val Loss: 0.5170, Val Acc: 0.7817\n",
      "Fold 2, Epoch 33/50 - Train Loss: 0.4073, Train Acc: 0.8796, Val Loss: 0.4523, Val Acc: 0.8239\n",
      "Fold 2, Epoch 34/50 - Train Loss: 0.3942, Train Acc: 0.8389, Val Loss: 0.5009, Val Acc: 0.7817\n",
      "Fold 2, Epoch 35/50 - Train Loss: 0.3830, Train Acc: 0.8956, Val Loss: 0.4486, Val Acc: 0.8099\n",
      "Fold 2, Epoch 36/50 - Train Loss: 0.3526, Train Acc: 0.9044, Val Loss: 0.4767, Val Acc: 0.8028\n",
      "Fold 2, Epoch 37/50 - Train Loss: 0.3292, Train Acc: 0.8814, Val Loss: 0.4860, Val Acc: 0.8028\n",
      "Fold 2, Epoch 38/50 - Train Loss: 0.3753, Train Acc: 0.8991, Val Loss: 0.5313, Val Acc: 0.7746\n",
      "Fold 2, Epoch 39/50 - Train Loss: 0.3607, Train Acc: 0.9080, Val Loss: 0.4989, Val Acc: 0.8099\n",
      "Fold 2, Epoch 40/50 - Train Loss: 0.3480, Train Acc: 0.9062, Val Loss: 0.5075, Val Acc: 0.7746\n",
      "Fold 2, Epoch 41/50 - Train Loss: 0.3055, Train Acc: 0.9009, Val Loss: 0.4924, Val Acc: 0.8169\n",
      "Fold 2, Epoch 42/50 - Train Loss: 0.3140, Train Acc: 0.9080, Val Loss: 0.5035, Val Acc: 0.8169\n",
      "Fold 2, Epoch 43/50 - Train Loss: 0.3255, Train Acc: 0.8442, Val Loss: 0.5871, Val Acc: 0.7817\n",
      "Fold 2, Epoch 44/50 - Train Loss: 0.3895, Train Acc: 0.9204, Val Loss: 0.4906, Val Acc: 0.8169\n",
      "Fold 2, Epoch 45/50 - Train Loss: 0.3219, Train Acc: 0.9150, Val Loss: 0.5100, Val Acc: 0.8239\n",
      "Fold 2, Epoch 46/50 - Train Loss: 0.2963, Train Acc: 0.8991, Val Loss: 0.4975, Val Acc: 0.8169\n",
      "Fold 2, Epoch 47/50 - Train Loss: 0.2890, Train Acc: 0.9451, Val Loss: 0.5607, Val Acc: 0.8028\n",
      "Fold 2, Epoch 48/50 - Train Loss: 0.2738, Train Acc: 0.9115, Val Loss: 0.6058, Val Acc: 0.7676\n",
      "Fold 2, Epoch 49/50 - Train Loss: 0.2741, Train Acc: 0.9221, Val Loss: 0.5821, Val Acc: 0.8028\n",
      "Fold 2, Epoch 50/50 - Train Loss: 0.2367, Train Acc: 0.9363, Val Loss: 0.5221, Val Acc: 0.8521\n",
      "Fold 3/5\n",
      "Fold 3, Epoch 1/50 - Train Loss: 0.7326, Train Acc: 0.5000, Val Loss: 0.7633, Val Acc: 0.5035\n",
      "Fold 3, Epoch 2/50 - Train Loss: 0.6370, Train Acc: 0.5742, Val Loss: 0.6750, Val Acc: 0.5816\n",
      "Fold 3, Epoch 3/50 - Train Loss: 0.6168, Train Acc: 0.6184, Val Loss: 0.7076, Val Acc: 0.5674\n",
      "Fold 3, Epoch 4/50 - Train Loss: 0.6233, Train Acc: 0.6926, Val Loss: 0.6111, Val Acc: 0.7234\n",
      "Fold 3, Epoch 5/50 - Train Loss: 0.5936, Train Acc: 0.6890, Val Loss: 0.6070, Val Acc: 0.6312\n",
      "Fold 3, Epoch 6/50 - Train Loss: 0.5720, Train Acc: 0.7279, Val Loss: 0.5539, Val Acc: 0.7660\n",
      "Fold 3, Epoch 7/50 - Train Loss: 0.5513, Train Acc: 0.7668, Val Loss: 0.5336, Val Acc: 0.7589\n",
      "Fold 3, Epoch 8/50 - Train Loss: 0.5382, Train Acc: 0.7615, Val Loss: 0.5211, Val Acc: 0.7872\n",
      "Fold 3, Epoch 9/50 - Train Loss: 0.5273, Train Acc: 0.7473, Val Loss: 0.5301, Val Acc: 0.7305\n",
      "Fold 3, Epoch 10/50 - Train Loss: 0.5052, Train Acc: 0.7774, Val Loss: 0.5175, Val Acc: 0.7447\n",
      "Fold 3, Epoch 11/50 - Train Loss: 0.5173, Train Acc: 0.7686, Val Loss: 0.6150, Val Acc: 0.7305\n",
      "Fold 3, Epoch 12/50 - Train Loss: 0.5277, Train Acc: 0.7968, Val Loss: 0.5123, Val Acc: 0.7730\n",
      "Fold 3, Epoch 13/50 - Train Loss: 0.4595, Train Acc: 0.8039, Val Loss: 0.5141, Val Acc: 0.7660\n",
      "Fold 3, Epoch 14/50 - Train Loss: 0.4514, Train Acc: 0.7986, Val Loss: 0.5154, Val Acc: 0.7730\n",
      "Fold 3, Epoch 15/50 - Train Loss: 0.4965, Train Acc: 0.8286, Val Loss: 0.4733, Val Acc: 0.8014\n",
      "Fold 3, Epoch 16/50 - Train Loss: 0.4773, Train Acc: 0.8092, Val Loss: 0.5334, Val Acc: 0.7518\n",
      "Fold 3, Epoch 17/50 - Train Loss: 0.4644, Train Acc: 0.8110, Val Loss: 0.5263, Val Acc: 0.7376\n",
      "Fold 3, Epoch 18/50 - Train Loss: 0.4539, Train Acc: 0.7862, Val Loss: 0.6316, Val Acc: 0.7163\n",
      "Fold 3, Epoch 19/50 - Train Loss: 0.4484, Train Acc: 0.8516, Val Loss: 0.4906, Val Acc: 0.8014\n",
      "Fold 3, Epoch 20/50 - Train Loss: 0.4282, Train Acc: 0.8339, Val Loss: 0.5206, Val Acc: 0.7801\n",
      "Fold 3, Epoch 21/50 - Train Loss: 0.4275, Train Acc: 0.7774, Val Loss: 0.6056, Val Acc: 0.7518\n",
      "Fold 3, Epoch 22/50 - Train Loss: 0.4416, Train Acc: 0.8587, Val Loss: 0.4498, Val Acc: 0.8369\n",
      "Fold 3, Epoch 23/50 - Train Loss: 0.4321, Train Acc: 0.8710, Val Loss: 0.4745, Val Acc: 0.8085\n",
      "Fold 3, Epoch 24/50 - Train Loss: 0.3886, Train Acc: 0.8516, Val Loss: 0.5627, Val Acc: 0.7801\n",
      "Fold 3, Epoch 25/50 - Train Loss: 0.3948, Train Acc: 0.8622, Val Loss: 0.5136, Val Acc: 0.8014\n",
      "Fold 3, Epoch 26/50 - Train Loss: 0.3884, Train Acc: 0.8640, Val Loss: 0.5024, Val Acc: 0.8014\n",
      "Fold 3, Epoch 27/50 - Train Loss: 0.3961, Train Acc: 0.8251, Val Loss: 0.6166, Val Acc: 0.7660\n",
      "Fold 3, Epoch 28/50 - Train Loss: 0.4003, Train Acc: 0.8728, Val Loss: 0.5258, Val Acc: 0.8085\n",
      "Fold 3, Epoch 29/50 - Train Loss: 0.3890, Train Acc: 0.7915, Val Loss: 0.6373, Val Acc: 0.7660\n",
      "Fold 3, Epoch 30/50 - Train Loss: 0.3575, Train Acc: 0.8746, Val Loss: 0.5241, Val Acc: 0.8014\n",
      "Fold 3, Epoch 31/50 - Train Loss: 0.3886, Train Acc: 0.8693, Val Loss: 0.5620, Val Acc: 0.8156\n",
      "Fold 3, Epoch 32/50 - Train Loss: 0.3775, Train Acc: 0.8816, Val Loss: 0.5611, Val Acc: 0.7660\n",
      "Fold 3, Epoch 33/50 - Train Loss: 0.3503, Train Acc: 0.8569, Val Loss: 0.6194, Val Acc: 0.7589\n",
      "Fold 3, Epoch 34/50 - Train Loss: 0.3493, Train Acc: 0.8905, Val Loss: 0.5442, Val Acc: 0.7730\n",
      "Fold 3, Epoch 35/50 - Train Loss: 0.3779, Train Acc: 0.8657, Val Loss: 0.5867, Val Acc: 0.7872\n",
      "Fold 3, Epoch 36/50 - Train Loss: 0.3390, Train Acc: 0.8887, Val Loss: 0.5089, Val Acc: 0.7872\n",
      "Fold 3, Epoch 37/50 - Train Loss: 0.3511, Train Acc: 0.8799, Val Loss: 0.5956, Val Acc: 0.8085\n",
      "Fold 3, Epoch 38/50 - Train Loss: 0.3207, Train Acc: 0.8905, Val Loss: 0.5670, Val Acc: 0.8156\n",
      "Fold 3, Epoch 39/50 - Train Loss: 0.3211, Train Acc: 0.8958, Val Loss: 0.5421, Val Acc: 0.7801\n",
      "Fold 3, Epoch 40/50 - Train Loss: 0.3651, Train Acc: 0.8127, Val Loss: 0.7962, Val Acc: 0.7305\n",
      "Fold 3, Epoch 41/50 - Train Loss: 0.3768, Train Acc: 0.8887, Val Loss: 0.5981, Val Acc: 0.7660\n",
      "Fold 3, Epoch 42/50 - Train Loss: 0.3279, Train Acc: 0.9134, Val Loss: 0.4941, Val Acc: 0.8014\n",
      "Fold 3, Epoch 43/50 - Train Loss: 0.2941, Train Acc: 0.8763, Val Loss: 0.6533, Val Acc: 0.7730\n",
      "Fold 3, Epoch 44/50 - Train Loss: 0.3149, Train Acc: 0.9276, Val Loss: 0.5405, Val Acc: 0.7943\n",
      "Fold 3, Epoch 45/50 - Train Loss: 0.3198, Train Acc: 0.8958, Val Loss: 0.6362, Val Acc: 0.7872\n",
      "Fold 3, Epoch 46/50 - Train Loss: 0.2949, Train Acc: 0.9134, Val Loss: 0.6237, Val Acc: 0.7943\n",
      "Fold 3, Epoch 47/50 - Train Loss: 0.3098, Train Acc: 0.9293, Val Loss: 0.5702, Val Acc: 0.7872\n",
      "Fold 3, Epoch 48/50 - Train Loss: 0.3025, Train Acc: 0.8993, Val Loss: 0.6893, Val Acc: 0.7589\n",
      "Fold 3, Epoch 49/50 - Train Loss: 0.2638, Train Acc: 0.8993, Val Loss: 0.6454, Val Acc: 0.7589\n",
      "Fold 3, Epoch 50/50 - Train Loss: 0.2843, Train Acc: 0.9346, Val Loss: 0.6321, Val Acc: 0.7730\n",
      "Fold 4/5\n",
      "Fold 4, Epoch 1/50 - Train Loss: 0.6997, Train Acc: 0.4753, Val Loss: 0.7662, Val Acc: 0.5390\n",
      "Fold 4, Epoch 2/50 - Train Loss: 0.6458, Train Acc: 0.5124, Val Loss: 0.6888, Val Acc: 0.5106\n",
      "Fold 4, Epoch 3/50 - Train Loss: 0.6281, Train Acc: 0.5071, Val Loss: 0.7230, Val Acc: 0.5106\n",
      "Fold 4, Epoch 4/50 - Train Loss: 0.6146, Train Acc: 0.5530, Val Loss: 0.7300, Val Acc: 0.5532\n",
      "Fold 4, Epoch 5/50 - Train Loss: 0.5874, Train Acc: 0.6943, Val Loss: 0.6061, Val Acc: 0.6525\n",
      "Fold 4, Epoch 6/50 - Train Loss: 0.5816, Train Acc: 0.7208, Val Loss: 0.5900, Val Acc: 0.6667\n",
      "Fold 4, Epoch 7/50 - Train Loss: 0.5639, Train Acc: 0.7350, Val Loss: 0.5194, Val Acc: 0.7447\n",
      "Fold 4, Epoch 8/50 - Train Loss: 0.5510, Train Acc: 0.7633, Val Loss: 0.4727, Val Acc: 0.7872\n",
      "Fold 4, Epoch 9/50 - Train Loss: 0.5313, Train Acc: 0.7562, Val Loss: 0.4770, Val Acc: 0.7730\n",
      "Fold 4, Epoch 10/50 - Train Loss: 0.5299, Train Acc: 0.7544, Val Loss: 0.4507, Val Acc: 0.7801\n",
      "Fold 4, Epoch 11/50 - Train Loss: 0.4976, Train Acc: 0.7809, Val Loss: 0.4316, Val Acc: 0.8156\n",
      "Fold 4, Epoch 12/50 - Train Loss: 0.5190, Train Acc: 0.7562, Val Loss: 0.4669, Val Acc: 0.7801\n",
      "Fold 4, Epoch 13/50 - Train Loss: 0.4965, Train Acc: 0.7827, Val Loss: 0.4379, Val Acc: 0.8156\n",
      "Fold 4, Epoch 14/50 - Train Loss: 0.5148, Train Acc: 0.7986, Val Loss: 0.4476, Val Acc: 0.8085\n",
      "Fold 4, Epoch 15/50 - Train Loss: 0.4761, Train Acc: 0.7827, Val Loss: 0.4635, Val Acc: 0.7872\n",
      "Fold 4, Epoch 16/50 - Train Loss: 0.4795, Train Acc: 0.7880, Val Loss: 0.4710, Val Acc: 0.8227\n",
      "Fold 4, Epoch 17/50 - Train Loss: 0.4542, Train Acc: 0.7986, Val Loss: 0.4314, Val Acc: 0.8369\n",
      "Fold 4, Epoch 18/50 - Train Loss: 0.4567, Train Acc: 0.8269, Val Loss: 0.4555, Val Acc: 0.7872\n",
      "Fold 4, Epoch 19/50 - Train Loss: 0.4975, Train Acc: 0.8198, Val Loss: 0.4520, Val Acc: 0.7872\n",
      "Fold 4, Epoch 20/50 - Train Loss: 0.4380, Train Acc: 0.8092, Val Loss: 0.5051, Val Acc: 0.7730\n",
      "Fold 4, Epoch 21/50 - Train Loss: 0.4372, Train Acc: 0.8357, Val Loss: 0.4361, Val Acc: 0.8227\n",
      "Fold 4, Epoch 22/50 - Train Loss: 0.4318, Train Acc: 0.8392, Val Loss: 0.4559, Val Acc: 0.8014\n",
      "Fold 4, Epoch 23/50 - Train Loss: 0.4313, Train Acc: 0.8357, Val Loss: 0.4740, Val Acc: 0.7872\n",
      "Fold 4, Epoch 24/50 - Train Loss: 0.4315, Train Acc: 0.8569, Val Loss: 0.4422, Val Acc: 0.8156\n",
      "Fold 4, Epoch 25/50 - Train Loss: 0.4307, Train Acc: 0.7809, Val Loss: 0.5934, Val Acc: 0.7589\n",
      "Fold 4, Epoch 26/50 - Train Loss: 0.4056, Train Acc: 0.8251, Val Loss: 0.4528, Val Acc: 0.8156\n",
      "Fold 4, Epoch 27/50 - Train Loss: 0.4410, Train Acc: 0.8357, Val Loss: 0.4406, Val Acc: 0.8014\n",
      "Fold 4, Epoch 28/50 - Train Loss: 0.3847, Train Acc: 0.8410, Val Loss: 0.4503, Val Acc: 0.8369\n",
      "Fold 4, Epoch 29/50 - Train Loss: 0.3843, Train Acc: 0.8198, Val Loss: 0.5218, Val Acc: 0.8014\n",
      "Fold 4, Epoch 30/50 - Train Loss: 0.3912, Train Acc: 0.8640, Val Loss: 0.4720, Val Acc: 0.7872\n",
      "Fold 4, Epoch 31/50 - Train Loss: 0.3625, Train Acc: 0.8781, Val Loss: 0.4486, Val Acc: 0.8298\n",
      "Fold 4, Epoch 32/50 - Train Loss: 0.3876, Train Acc: 0.8587, Val Loss: 0.4918, Val Acc: 0.8085\n",
      "Fold 4, Epoch 33/50 - Train Loss: 0.4058, Train Acc: 0.8763, Val Loss: 0.4519, Val Acc: 0.8227\n",
      "Fold 4, Epoch 34/50 - Train Loss: 0.3884, Train Acc: 0.8693, Val Loss: 0.4704, Val Acc: 0.7872\n",
      "Fold 4, Epoch 35/50 - Train Loss: 0.4023, Train Acc: 0.8569, Val Loss: 0.4925, Val Acc: 0.7943\n",
      "Fold 4, Epoch 36/50 - Train Loss: 0.3583, Train Acc: 0.8799, Val Loss: 0.4982, Val Acc: 0.8085\n",
      "Fold 4, Epoch 37/50 - Train Loss: 0.3392, Train Acc: 0.8940, Val Loss: 0.4877, Val Acc: 0.8085\n",
      "Fold 4, Epoch 38/50 - Train Loss: 0.3588, Train Acc: 0.8975, Val Loss: 0.4554, Val Acc: 0.8298\n",
      "Fold 4, Epoch 39/50 - Train Loss: 0.3520, Train Acc: 0.8428, Val Loss: 0.5398, Val Acc: 0.7589\n",
      "Fold 4, Epoch 40/50 - Train Loss: 0.3547, Train Acc: 0.9081, Val Loss: 0.4737, Val Acc: 0.8085\n",
      "Fold 4, Epoch 41/50 - Train Loss: 0.3305, Train Acc: 0.8958, Val Loss: 0.4557, Val Acc: 0.7943\n",
      "Fold 4, Epoch 42/50 - Train Loss: 0.3399, Train Acc: 0.8569, Val Loss: 0.5444, Val Acc: 0.7943\n",
      "Fold 4, Epoch 43/50 - Train Loss: 0.3285, Train Acc: 0.8763, Val Loss: 0.4683, Val Acc: 0.8014\n",
      "Fold 4, Epoch 44/50 - Train Loss: 0.2960, Train Acc: 0.9205, Val Loss: 0.4168, Val Acc: 0.8440\n",
      "Fold 4, Epoch 45/50 - Train Loss: 0.3139, Train Acc: 0.9081, Val Loss: 0.4783, Val Acc: 0.8014\n",
      "Fold 4, Epoch 46/50 - Train Loss: 0.2978, Train Acc: 0.9170, Val Loss: 0.4541, Val Acc: 0.8582\n",
      "Fold 4, Epoch 47/50 - Train Loss: 0.2911, Train Acc: 0.8799, Val Loss: 0.5140, Val Acc: 0.8085\n",
      "Fold 4, Epoch 48/50 - Train Loss: 0.3396, Train Acc: 0.8816, Val Loss: 0.5759, Val Acc: 0.7801\n",
      "Fold 4, Epoch 49/50 - Train Loss: 0.2744, Train Acc: 0.9081, Val Loss: 0.5085, Val Acc: 0.8369\n",
      "Fold 4, Epoch 50/50 - Train Loss: 0.2987, Train Acc: 0.9329, Val Loss: 0.4974, Val Acc: 0.8085\n",
      "Fold 5/5\n",
      "Fold 5, Epoch 1/50 - Train Loss: 0.7835, Train Acc: 0.4664, Val Loss: 0.7795, Val Acc: 0.4539\n",
      "Fold 5, Epoch 2/50 - Train Loss: 0.6487, Train Acc: 0.5124, Val Loss: 0.7401, Val Acc: 0.5106\n",
      "Fold 5, Epoch 3/50 - Train Loss: 0.5982, Train Acc: 0.5883, Val Loss: 0.7196, Val Acc: 0.4894\n",
      "Fold 5, Epoch 4/50 - Train Loss: 0.5867, Train Acc: 0.6519, Val Loss: 0.7168, Val Acc: 0.5532\n",
      "Fold 5, Epoch 5/50 - Train Loss: 0.5958, Train Acc: 0.6890, Val Loss: 0.7116, Val Acc: 0.6028\n",
      "Fold 5, Epoch 6/50 - Train Loss: 0.5733, Train Acc: 0.7562, Val Loss: 0.6337, Val Acc: 0.7021\n",
      "Fold 5, Epoch 7/50 - Train Loss: 0.5569, Train Acc: 0.7509, Val Loss: 0.6226, Val Acc: 0.6525\n",
      "Fold 5, Epoch 8/50 - Train Loss: 0.5314, Train Acc: 0.7739, Val Loss: 0.6552, Val Acc: 0.6525\n",
      "Fold 5, Epoch 9/50 - Train Loss: 0.5041, Train Acc: 0.7986, Val Loss: 0.6325, Val Acc: 0.7163\n",
      "Fold 5, Epoch 10/50 - Train Loss: 0.4907, Train Acc: 0.7898, Val Loss: 0.6808, Val Acc: 0.6738\n",
      "Fold 5, Epoch 11/50 - Train Loss: 0.4814, Train Acc: 0.7880, Val Loss: 0.7497, Val Acc: 0.6596\n",
      "Fold 5, Epoch 12/50 - Train Loss: 0.5025, Train Acc: 0.7862, Val Loss: 0.7230, Val Acc: 0.6525\n",
      "Fold 5, Epoch 13/50 - Train Loss: 0.4832, Train Acc: 0.8269, Val Loss: 0.7136, Val Acc: 0.6950\n",
      "Fold 5, Epoch 14/50 - Train Loss: 0.5217, Train Acc: 0.8127, Val Loss: 0.6791, Val Acc: 0.6738\n",
      "Fold 5, Epoch 15/50 - Train Loss: 0.4705, Train Acc: 0.7933, Val Loss: 0.7844, Val Acc: 0.6454\n",
      "Fold 5, Epoch 16/50 - Train Loss: 0.4246, Train Acc: 0.8304, Val Loss: 0.6535, Val Acc: 0.7518\n",
      "Fold 5, Epoch 17/50 - Train Loss: 0.4454, Train Acc: 0.7933, Val Loss: 0.8314, Val Acc: 0.6525\n",
      "Fold 5, Epoch 18/50 - Train Loss: 0.4245, Train Acc: 0.8410, Val Loss: 0.7267, Val Acc: 0.6809\n",
      "Fold 5, Epoch 19/50 - Train Loss: 0.4440, Train Acc: 0.8375, Val Loss: 0.7136, Val Acc: 0.7518\n",
      "Fold 5, Epoch 20/50 - Train Loss: 0.4004, Train Acc: 0.7633, Val Loss: 0.9069, Val Acc: 0.6170\n",
      "Fold 5, Epoch 21/50 - Train Loss: 0.4642, Train Acc: 0.8640, Val Loss: 0.7005, Val Acc: 0.7234\n",
      "Fold 5, Epoch 22/50 - Train Loss: 0.4088, Train Acc: 0.8534, Val Loss: 0.7215, Val Acc: 0.7234\n",
      "Fold 5, Epoch 23/50 - Train Loss: 0.3949, Train Acc: 0.8498, Val Loss: 0.7061, Val Acc: 0.7376\n",
      "Fold 5, Epoch 24/50 - Train Loss: 0.4094, Train Acc: 0.8816, Val Loss: 0.6585, Val Acc: 0.7518\n",
      "Fold 5, Epoch 25/50 - Train Loss: 0.3765, Train Acc: 0.8039, Val Loss: 0.8478, Val Acc: 0.6454\n",
      "Fold 5, Epoch 26/50 - Train Loss: 0.3968, Train Acc: 0.8869, Val Loss: 0.7111, Val Acc: 0.7305\n",
      "Fold 5, Epoch 27/50 - Train Loss: 0.3680, Train Acc: 0.8852, Val Loss: 0.6933, Val Acc: 0.7376\n",
      "Fold 5, Epoch 28/50 - Train Loss: 0.3444, Train Acc: 0.8445, Val Loss: 0.8481, Val Acc: 0.7021\n",
      "Fold 5, Epoch 29/50 - Train Loss: 0.3609, Train Acc: 0.8940, Val Loss: 0.7372, Val Acc: 0.7305\n",
      "Fold 5, Epoch 30/50 - Train Loss: 0.3504, Train Acc: 0.8763, Val Loss: 0.8024, Val Acc: 0.7092\n",
      "Fold 5, Epoch 31/50 - Train Loss: 0.3625, Train Acc: 0.8710, Val Loss: 0.7457, Val Acc: 0.7447\n",
      "Fold 5, Epoch 32/50 - Train Loss: 0.3255, Train Acc: 0.8958, Val Loss: 0.6981, Val Acc: 0.7234\n",
      "Fold 5, Epoch 33/50 - Train Loss: 0.3106, Train Acc: 0.8781, Val Loss: 0.8210, Val Acc: 0.7092\n",
      "Fold 5, Epoch 34/50 - Train Loss: 0.3191, Train Acc: 0.8869, Val Loss: 0.7520, Val Acc: 0.7376\n",
      "Fold 5, Epoch 35/50 - Train Loss: 0.3178, Train Acc: 0.8887, Val Loss: 0.7054, Val Acc: 0.7589\n",
      "Fold 5, Epoch 36/50 - Train Loss: 0.2942, Train Acc: 0.8940, Val Loss: 0.8233, Val Acc: 0.7234\n",
      "Fold 5, Epoch 37/50 - Train Loss: 0.3303, Train Acc: 0.9028, Val Loss: 0.7958, Val Acc: 0.7376\n",
      "Fold 5, Epoch 38/50 - Train Loss: 0.3012, Train Acc: 0.8286, Val Loss: 0.9713, Val Acc: 0.7092\n",
      "Fold 5, Epoch 39/50 - Train Loss: 0.3794, Train Acc: 0.9011, Val Loss: 0.7977, Val Acc: 0.7234\n",
      "Fold 5, Epoch 40/50 - Train Loss: 0.3682, Train Acc: 0.8834, Val Loss: 0.6913, Val Acc: 0.7376\n",
      "Fold 5, Epoch 41/50 - Train Loss: 0.3386, Train Acc: 0.8693, Val Loss: 0.8167, Val Acc: 0.7234\n",
      "Fold 5, Epoch 42/50 - Train Loss: 0.3094, Train Acc: 0.9081, Val Loss: 0.7492, Val Acc: 0.7589\n",
      "Fold 5, Epoch 43/50 - Train Loss: 0.3048, Train Acc: 0.9205, Val Loss: 0.7522, Val Acc: 0.7518\n",
      "Fold 5, Epoch 44/50 - Train Loss: 0.2872, Train Acc: 0.8922, Val Loss: 0.7915, Val Acc: 0.7518\n",
      "Fold 5, Epoch 45/50 - Train Loss: 0.2668, Train Acc: 0.9099, Val Loss: 0.7940, Val Acc: 0.7234\n",
      "Fold 5, Epoch 46/50 - Train Loss: 0.3109, Train Acc: 0.9276, Val Loss: 0.7378, Val Acc: 0.7305\n",
      "Fold 5, Epoch 47/50 - Train Loss: 0.2847, Train Acc: 0.8975, Val Loss: 0.8385, Val Acc: 0.7305\n",
      "Fold 5, Epoch 48/50 - Train Loss: 0.2767, Train Acc: 0.8958, Val Loss: 0.7950, Val Acc: 0.7376\n",
      "Fold 5, Epoch 49/50 - Train Loss: 0.2575, Train Acc: 0.9187, Val Loss: 0.8477, Val Acc: 0.7518\n",
      "Fold 5, Epoch 50/50 - Train Loss: 0.2652, Train Acc: 0.9293, Val Loss: 0.8466, Val Acc: 0.7589\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fold_models = []  \n",
    "fold_results = []  \n",
    "\n",
    "train_accuracies_per_fold = []\n",
    "val_accuracies_per_fold = []\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(stratified_kfold.split(np.zeros(len(remaining_labels)), remaining_labels)):\n",
    "    print(f\"Fold {fold + 1}/{k_folds}\")\n",
    "\n",
    "    train_indices = [remaining_indices[i] for i in train_indices]\n",
    "    val_indices = [remaining_indices[i] for i in val_indices]\n",
    "\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = ConformerTinyBinary().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_path = os.path.join(model_dir, f\"best_model_fold{fold + 1}.pth\")\n",
    "\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    train_losses = []  \n",
    "    val_losses = []    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        _, train_acc = validate(model, train_loader, criterion, device)\n",
    "        train_accuracies.append(train_acc)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        \n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        val_accuracies.append(val_acc)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Fold {fold + 1}, Epoch {epoch + 1}/{num_epochs} - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_model(epoch, model, optimizer, best_model_path, best=True)\n",
    "\n",
    "    \n",
    "    train_accuracies_per_fold.append(train_accuracies)\n",
    "    val_accuracies_per_fold.append(val_accuracies)\n",
    "\n",
    "    \n",
    "    fold_results.append({\n",
    "        \"fold\": fold + 1,\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"train_accuracies\": train_accuracies,\n",
    "        \"val_accuracies\": val_accuracies,\n",
    "        \"best_val_loss\": best_val_loss\n",
    "    })\n",
    "\n",
    "    \n",
    "    model.load_state_dict(torch.load(best_model_path)['model_state_dict'])\n",
    "    fold_models.append(model)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on the test set with all fold models...\n",
      "\n",
      "Final Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80        40\n",
      "           1       0.79      0.79      0.79        39\n",
      "\n",
      "    accuracy                           0.80        79\n",
      "   macro avg       0.80      0.80      0.80        79\n",
      "weighted avg       0.80      0.80      0.80        79\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nEvaluating on the test set with all fold models...\")\n",
    "test_predictions = np.zeros(len(test_dataset))  \n",
    "test_true_labels = []  \n",
    "\n",
    "\n",
    "for model in fold_models:\n",
    "    model.eval()\n",
    "    fold_test_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            probabilities = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()  \n",
    "            fold_test_scores.extend(probabilities)\n",
    "\n",
    "            \n",
    "            if len(test_true_labels) < len(test_dataset):\n",
    "                test_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    \n",
    "    fold_test_scores = np.array(fold_test_scores)\n",
    "\n",
    "    if len(fold_test_scores) != len(test_dataset):\n",
    "        raise ValueError(f\"Model predictions length mismatch: expected {len(test_dataset)}, got {len(fold_test_scores)}\")\n",
    "\n",
    "    \n",
    "    test_predictions += fold_test_scores\n",
    "\n",
    "\n",
    "test_true_labels = np.array(test_true_labels)\n",
    "\n",
    "\n",
    "test_predictions /= len(fold_models)\n",
    "\n",
    "if len(test_true_labels) != len(test_predictions):\n",
    "    raise ValueError(\"Mismatch between test labels and predictions!\")\n",
    "\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(test_true_labels, test_predictions)\n",
    "roc_auc = roc_auc_score(test_true_labels, test_predictions)\n",
    "\n",
    "\n",
    "min_tpr = 0.8  \n",
    "partial_auc = custom_metric(test_true_labels, test_predictions, min_tpr=min_tpr)\n",
    "\n",
    "y_pred = (test_predictions >= 0.5).astype(int)  \n",
    "\n",
    "print(\"\\nFinal Classification Report:\")\n",
    "print(classification_report(test_true_labels, y_pred, target_names=dataset.classes))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved loss and accuracy plots for Fold 1 at scratch_test_augmentless_kfold/saved_models\\fold_1_loss_accuracy.png\n",
      "Saved loss and accuracy plots for Fold 2 at scratch_test_augmentless_kfold/saved_models\\fold_2_loss_accuracy.png\n",
      "Saved loss and accuracy plots for Fold 3 at scratch_test_augmentless_kfold/saved_models\\fold_3_loss_accuracy.png\n",
      "Saved loss and accuracy plots for Fold 4 at scratch_test_augmentless_kfold/saved_models\\fold_4_loss_accuracy.png\n",
      "Saved loss and accuracy plots for Fold 5 at scratch_test_augmentless_kfold/saved_models\\fold_5_loss_accuracy.png\n",
      "Saved ROC curve at scratch_test_augmentless_kfold/saved_models\\ROC_curve_with_custom_metric.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for fold, (train_accuracies, val_accuracies) in enumerate(zip(train_accuracies_per_fold, val_accuracies_per_fold)):\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    train_losses = fold_results[fold].get(\"train_losses\", [])\n",
    "    val_losses = fold_results[fold].get(\"val_losses\", [])\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Train Loss\", marker='o')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Validation Loss\", marker='o', linestyle='--')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Fold {fold + 1}: Train vs. Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label=\"Train Accuracy\", marker='o')\n",
    "    plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label=\"Validation Accuracy\", marker='o', linestyle='--')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(f\"Fold {fold + 1}: Train vs. Validation Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    \n",
    "    fold_plot_path = os.path.join(model_dir, f\"fold_{fold + 1}_loss_accuracy.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fold_plot_path, bbox_inches=\"tight\")\n",
    "    print(f\"Saved loss and accuracy plots for Fold {fold + 1} at {fold_plot_path}\")\n",
    "    plt.close()  \n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.4f}\", linewidth=2)\n",
    "\n",
    "\n",
    "partial_fpr = fpr[fpr >= 1 - min_tpr]  \n",
    "partial_tpr = tpr[fpr >= 1 - min_tpr]  \n",
    "plt.fill_between(partial_fpr, partial_tpr, alpha=0.2, label=f\"pAUC (TPR ≥ {min_tpr:.1f}) = {partial_auc:.4f}\")\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve (Test Set) with Custom Metric\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "\n",
    "roc_curve_path = os.path.join(model_dir, \"ROC_curve_with_custom_metric.png\")\n",
    "plt.savefig(roc_curve_path, bbox_inches=\"tight\")\n",
    "print(f\"Saved ROC curve at {roc_curve_path}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters and architecture saved to scratch_test_augmentless_kfold/saved_models\\model_params.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params_file_path = os.path.join(model_dir, \"model_params.txt\")\n",
    "with open(params_file_path, \"w\") as f:\n",
    "    f.write(\"Model Architecture:\\n\")\n",
    "    f.write(str(model) + \"\\n\\n\")  \n",
    "    \n",
    "    f.write(\"Hyperparameters:\\n\")\n",
    "    f.write(f\"Batch Size: {batch_size}\\n\")\n",
    "    f.write(f\"Learning Rate: {learning_rate}\\n\")\n",
    "    f.write(f\"Number of Epochs: {num_epochs}\\n\")\n",
    "    f.write(f\"Optimizer: Adam\\n\")\n",
    "    f.write(f\"Loss Function: CrossEntropyLoss\\n\")\n",
    "    \n",
    "print(f\"Model parameters and architecture saved to {params_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
