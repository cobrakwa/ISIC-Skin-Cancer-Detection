{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu118\n",
      "True\n",
      "NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davis/miniconda3/envs/cv/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.22 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision.models import resnet50, ResNet50_Weights, resnet101, ResNet101_Weights , swin_t,efficientnet_b3,efficientnet_b2\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "from sklearn.metrics import roc_auc_score, classification_report,precision_recall_fscore_support,roc_curve, auc\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.preprocessing import  OneHotEncoder\n",
    "import polars as pl\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_dataset_with_augmentations_balanced(df, test_size=0.1, val_size=0.11, random_state=42):\n",
    "    \"\"\"\n",
    "    Split dataset ensuring equal class distribution in all splits while keeping\n",
    "    original images and their augmentations together.\n",
    "    \"\"\"\n",
    "    def get_group_sizes(group_df):\n",
    "        return len(group_df)\n",
    "    \n",
    "    grouped = df.groupby('isic_id')\n",
    "    group_sizes = grouped.apply(get_group_sizes)\n",
    "    \n",
    "    size_df = pd.DataFrame({\n",
    "        'isic_id': group_sizes.index,\n",
    "        'group_size': group_sizes.values,\n",
    "        'label': [grouped.get_group(name)['label'].iloc[0] for name in group_sizes.index]\n",
    "    })\n",
    "    \n",
    "    class_0_groups = size_df[size_df['label'] == 0]\n",
    "    class_1_groups = size_df[size_df['label'] == 1]\n",
    "    \n",
    "    min_class_total = min(\n",
    "        class_0_groups['group_size'].sum(),\n",
    "        class_1_groups['group_size'].sum()\n",
    "    )\n",
    "    \n",
    "    total_samples = min_class_total * 2 \n",
    "    n_test = int(total_samples * test_size)\n",
    "    n_val = int((total_samples - n_test) * val_size)\n",
    "    n_train = total_samples - n_test - n_val\n",
    "    \n",
    "    n_test = (n_test // 2) * 2\n",
    "    n_val = (n_val // 2) * 2\n",
    "    n_train = total_samples - n_test - n_val\n",
    "    \n",
    "    def sample_groups_to_size(class_groups, target_size):\n",
    "        \"\"\"Sample groups to achieve target size including augmentations\"\"\"\n",
    "        sampled_groups = []\n",
    "        current_size = 0\n",
    "        \n",
    "        class_groups = class_groups.sort_values('group_size')\n",
    "        \n",
    "        for _, group in class_groups.iterrows():\n",
    "            if current_size + group['group_size'] <= target_size:\n",
    "                sampled_groups.append(group['isic_id'])\n",
    "                current_size += group['group_size']\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        return sampled_groups\n",
    "    \n",
    "    for target_class in [0, 1]:\n",
    "        class_groups = size_df[size_df['label'] == target_class]\n",
    "        \n",
    "        # Sample for test set\n",
    "        test_names = sample_groups_to_size(class_groups, n_test // 2)\n",
    "        remaining_groups = class_groups[~class_groups['isic_id'].isin(test_names)]\n",
    "        \n",
    "        # Sample for validation set\n",
    "        val_names = sample_groups_to_size(remaining_groups, n_val // 2)\n",
    "        remaining_groups = remaining_groups[~remaining_groups['isic_id'].isin(val_names)]\n",
    "        \n",
    "        # Sample for train set\n",
    "        train_names = sample_groups_to_size(remaining_groups, n_train // 2)\n",
    "        \n",
    "        if target_class == 0:\n",
    "            test_names_0, val_names_0, train_names_0 = test_names, val_names, train_names\n",
    "        else:\n",
    "            test_names_1, val_names_1, train_names_1 = test_names, val_names, train_names\n",
    "    \n",
    "    test_names = test_names_0 + test_names_1\n",
    "    val_names = val_names_0 + val_names_1\n",
    "    train_names = train_names_0 + train_names_1\n",
    "    \n",
    "    test_df = df[df['isic_id'].isin(test_names)].copy()\n",
    "    val_df = df[df['isic_id'].isin(val_names)].copy()\n",
    "    train_df = df[df['isic_id'].isin(train_names)].copy()\n",
    "    \n",
    "    try:\n",
    "        test_df.to_csv('test_fromtrain_10percent.csv', index=False)\n",
    "        print(\"Successfully saved test set to CSV\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving test CSV: {str(e)}\")\n",
    "    stats = {\n",
    "        'train': {\n",
    "            'size': len(train_df),\n",
    "            'distribution': train_df['label'].value_counts().to_dict()\n",
    "        },\n",
    "        'val': {\n",
    "            'size': len(val_df),\n",
    "            'distribution': val_df['label'].value_counts().to_dict()\n",
    "        },\n",
    "        'test': {\n",
    "            'size': len(test_df),\n",
    "            'distribution': test_df['label'].value_counts().to_dict()\n",
    "        }\n",
    "    }\n",
    "        \n",
    "    return train_df, val_df, test_df,stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionFusion(nn.Module):\n",
    "    def __init__(self, img_dim, metadata_dim):\n",
    "        super().__init__()\n",
    "        self.img_proj = nn.Linear(img_dim, 256)     \n",
    "        self.key_proj = nn.Linear(metadata_dim, 256)  \n",
    "        self.value_proj = nn.Linear(metadata_dim, 256) \n",
    "        self.output_proj = nn.Linear(256, img_dim)\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(256)\n",
    "        \n",
    "    def forward(self, img_features, metadata):\n",
    "        Q = self.img_proj(img_features).unsqueeze(1)     # [B, 1, 256]\n",
    "        K = self.key_proj(metadata).unsqueeze(1)         # [B, 1, 256]\n",
    "        V = self.value_proj(metadata).unsqueeze(1)       # [B, 1, 256]\n",
    "        \n",
    "        attention = torch.bmm(Q, K.transpose(1, 2)) / math.sqrt(256)\n",
    "        attention = torch.softmax(attention, dim=-1)\n",
    "        \n",
    "        attended = torch.bmm(attention, V).squeeze(1)  # [B, 256]\n",
    "        attended = self.layer_norm(attended)\n",
    "        \n",
    "        output = self.output_proj(attended)\n",
    "        return img_features + output \n",
    "class HybridStructuredFusion(nn.Module):\n",
    "    def __init__(self, img_dim, metadata_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.meta_processor = nn.Sequential(\n",
    "            nn.Linear(metadata_dim, img_dim),\n",
    "            nn.LayerNorm(img_dim),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        self.img_processor = nn.Sequential(\n",
    "            nn.Linear(img_dim, img_dim),\n",
    "            nn.LayerNorm(img_dim),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(img_dim + metadata_dim, img_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(2 * img_dim, img_dim),\n",
    "            nn.LayerNorm(img_dim),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, img_features, metadata):\n",
    "        proc_meta = self.meta_processor(metadata) \n",
    "        proc_img = self.img_processor(img_features)\n",
    "        \n",
    "        gate = self.gate(torch.cat([img_features, metadata], dim=1))\n",
    "        \n",
    "        gated = torch.cat([\n",
    "            proc_img * gate,\n",
    "            proc_meta * (1 - gate)\n",
    "        ], dim=1)\n",
    "        \n",
    "        output = self.fusion(gated)\n",
    "        return img_features + output  \n",
    "class MetaDataFineTuneResnet(nn.Module):\n",
    "    def __init__(self, num_classes, metadata_dim, fusion_type='attention', freeze_layers=6):\n",
    "        super().__init__()\n",
    "        self.model = resnet101(weights='DEFAULT')\n",
    "        self.metadata_dim = metadata_dim\n",
    "        self.meta_dropout = nn.Dropout(0.3) \n",
    "\n",
    "        layers = list(self.model.named_children())\n",
    "        for name, child in layers[:freeze_layers]:\n",
    "            print(f\"Freezing layer: {name}\") \n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        num_ftrs = self.model.fc.in_features\n",
    "        self.model.fc = nn.Identity()\n",
    "   \n",
    "        if fusion_type == 'attention':\n",
    "            self.fusion = CrossAttentionFusion(num_ftrs, metadata_dim)\n",
    "        else:  \n",
    "            self.fusion = HybridStructuredFusion(num_ftrs, metadata_dim)\n",
    "        self.meta_normalizer = nn.BatchNorm1d(metadata_dim)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "   \n",
    "    def forward(self, x, metadata):\n",
    "        x = self.model(x)\n",
    "        metadata = self.meta_normalizer(metadata)\n",
    "        metadata = self.meta_dropout(metadata)\n",
    "        x = self.fusion(x, metadata)\n",
    "        return self.head(x)\n",
    "\n",
    "class BatchAlbumentation:\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def __call__(self, images, targets):\n",
    "        if isinstance(images, torch.Tensor):\n",
    "            images = images.numpy()\n",
    "            \n",
    "        if images.shape[1] == 3:  \n",
    "            images = np.transpose(images, (0, 2, 3, 1))\n",
    "        \n",
    "        augmented = [self.transform(image=img)['image'] for img in images]\n",
    "        aug_images = torch.stack(augmented)\n",
    "        \n",
    "        return aug_images, targets\n",
    "\n",
    "def create_transforms(image_size=224):\n",
    "\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(image_size, image_size),\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    transforms_train = A.Compose([\n",
    "        A.Transpose(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(\n",
    "            brightness_limit=0.2,\n",
    "            contrast_limit=0.2,\n",
    "            p=0.75\n",
    "         ),\n",
    "        A.OneOf([\n",
    "            A.MotionBlur(blur_limit=5),\n",
    "            A.MedianBlur(blur_limit=5),\n",
    "            A.GaussianBlur(blur_limit=5),\n",
    "            A.GaussNoise(var_limit=(5.0, 30.0)),\n",
    "        ], p=0.7),\n",
    "\n",
    "        A.OneOf([\n",
    "            A.OpticalDistortion(distort_limit=0.75),\n",
    "            A.GridDistortion(num_steps=5, distort_limit=0.75),\n",
    "            A.ElasticTransform(alpha=3),\n",
    "        ], p=0.7),\n",
    "\n",
    "        A.CLAHE(clip_limit=4.0, p=0.7),\n",
    "        A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.85),\n",
    "        A.Resize(image_size,image_size),\n",
    "\n",
    "        A.CoarseDropout(p=1.0),\n",
    "        A.Normalize()\n",
    "    ])\n",
    "\n",
    "    transforms_val = A.Compose([\n",
    "        A.Resize(image_size, image_size),\n",
    "        A.Normalize(),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    return transforms_train, transforms_val\n",
    "\n",
    "class PreAugmentedMetadataMelanomaDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 csv_file: pd.DataFrame,\n",
    "                 metadata_cols: list,\n",
    "                 transform: A.Compose = None,\n",
    "                 use_augmented: bool = True):\n",
    "        self.df = csv_file.copy()\n",
    "        self.transform = transform\n",
    "        self.metadata_cols = metadata_cols.copy()\n",
    "        self.target_size = 224\n",
    "        \n",
    "        for col in self.metadata_cols:\n",
    "            if col in self.df.columns: \n",
    "                self.df[col] = self.df[col].astype(np.float32)\n",
    "        \n",
    "        print(f\"Number of metadata features: {len(self.metadata_cols)}\")\n",
    "        print(f\"Sample metadata columns: {self.metadata_cols}...\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        image = cv2.imread(row['path'])\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Could not load image at path: {row['path']}\")\n",
    "            \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image)\n",
    "            image = transformed['image']\n",
    "        \n",
    "        metadata_values = []\n",
    "        for col in self.metadata_cols:\n",
    "            if col in self.df.columns:\n",
    "                val = row[col]\n",
    "                metadata_values.append(val if not pd.isna(val) else 0.0)\n",
    "            else:\n",
    "                metadata_values.append(0.0)  \n",
    "                \n",
    "        metadata = torch.tensor(metadata_values, dtype=torch.float32)\n",
    "        label = torch.tensor(row['label'], dtype=torch.long)\n",
    "        \n",
    "        return image, metadata, label\n",
    "class MetadataAugmentationDataLoader(torch.utils.data.DataLoader):\n",
    "    def __init__(self, dataset, \n",
    "                 batch_size=32, \n",
    "                 shuffle=True, \n",
    "                 num_workers=4, \n",
    "                 pin_memory=True,         \n",
    "                 prefetch_factor=3,\n",
    "                 persistent_workers=True,\n",
    "                 batch_transform=None):\n",
    "        \n",
    "        def collate_fn(batch):\n",
    "            images, metadata, labels = zip(*batch)\n",
    "            images = torch.stack([torch.tensor(img) for img in images])\n",
    "            metadata = torch.stack([meta for meta in metadata])\n",
    "            labels = torch.stack([label for label in labels])\n",
    "            return images, metadata, labels\n",
    "        \n",
    "        super().__init__(dataset, \n",
    "                         batch_size=batch_size, \n",
    "                         shuffle=shuffle, \n",
    "                         num_workers=num_workers,\n",
    "                         pin_memory=pin_memory, \n",
    "                         prefetch_factor=prefetch_factor,\n",
    "                         persistent_workers=persistent_workers,\n",
    "                         collate_fn=collate_fn)\n",
    "\n",
    "        self.batch_transform = batch_transform\n",
    "        \n",
    "    def __iter__(self):\n",
    "        iterator = super().__iter__()\n",
    "        if self.batch_transform is None:\n",
    "            return iterator\n",
    "        \n",
    "        return map(self._transform_batch, iterator)\n",
    "    \n",
    "    def _transform_batch(self, batch):\n",
    "        images, metadata, labels = batch\n",
    "        if self.batch_transform:\n",
    "            images, _ = self.batch_transform(images, labels)\n",
    "        return images, metadata, labels\n",
    "def custom_metric(y_true, y_pred, min_tpr=0.8):\n",
    "    \"\"\"\n",
    "    Calculate the partial AUC (pAUC) based on a minimum TPR threshold.\n",
    "\n",
    "    Args:\n",
    "        y_true (array): True binary labels.\n",
    "        y_pred (array): Predicted probabilities.\n",
    "        min_tpr (float): Minimum TPR threshold (default: 0.8).\n",
    "\n",
    "    Returns:\n",
    "        float: Scaled pAUC value.\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    max_fpr = 1 - min_tpr  \n",
    "    v_gt = abs(y_true - 1) \n",
    "    v_pred = 1.0 - y_pred  \n",
    "\n",
    "    pauc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "    pauc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (pauc_scaled - 0.5)\n",
    "\n",
    "    return pauc\n",
    "\n",
    "def train_model_with_metadata(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, patience):\n",
    "    \"\"\"\n",
    "    Training function with improved GPU utilization monitoring and debugging\n",
    "    \"\"\"\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "        print(f\"Device name: {torch.cuda.get_device_name()}\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    print(f\"Model device: {next(model.parameters()).device}\")\n",
    "    \n",
    "    best_val_auc = 0\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='max',\n",
    "        factor=0.5,\n",
    "        patience=patience,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "\n",
    "    print(\"\\nStarting main training loop...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Training Epoch {epoch+1}')\n",
    "        for batch_idx, (inputs, metadata, labels) in enumerate(pbar):\n",
    "            try:\n",
    "                inputs = inputs.to(device, non_blocking=True)\n",
    "                metadata = metadata.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                outputs = model(inputs, metadata)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                train_preds.extend(torch.softmax(outputs, dim=1)[:, 1].cpu().detach().numpy())\n",
    "                train_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                  \n",
    "                    pbar.set_postfix({\n",
    "                        'loss': f'{loss.item():.4f}',\n",
    "                 \n",
    "                    })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if len(train_labels) == 0:\n",
    "            print(\"No successful training batches in epoch\")\n",
    "            continue\n",
    "            \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_auc = roc_auc_score(train_labels, train_preds)\n",
    "        print(f'Epoch {epoch+1} - Loss: {epoch_loss:.4f}, AUC: {train_auc:.4f}')\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, metadata, labels in tqdm(val_loader, desc=f'Validation Epoch {epoch+1}'):\n",
    "                try:\n",
    "                    inputs = inputs.to(device)\n",
    "                    metadata = metadata.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    \n",
    "                    outputs = model(inputs, metadata)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    val_loss += loss.item() * inputs.size(0)\n",
    "                    val_preds.extend(torch.softmax(outputs, dim=1)[:, 1].cpu().numpy())\n",
    "                    val_labels.extend(labels.cpu().numpy())\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in validation batch: {str(e)}\")\n",
    "                    traceback.print_exc()\n",
    "                    continue\n",
    "        \n",
    "        if len(val_labels) == 0:\n",
    "            print(\"No successful validation batches in epoch\")\n",
    "            continue\n",
    "            \n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_auc = roc_auc_score(val_labels, val_preds)\n",
    "        \n",
    "        scheduler.step(val_auc)\n",
    "        \n",
    "        print(f'\\nEpoch {epoch+1} Results:')\n",
    "        print(f'Train Loss: {epoch_loss:.4f} Train AUC: {train_auc:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.4f} Val AUC: {val_auc:.4f}')\n",
    "        \n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_val_auc': best_val_auc,\n",
    "            }, 'best_model_with_metadata.pth')\n",
    "            print(f'New best model saved with validation AUC: {val_auc:.4f}')\n",
    "            no_improve_count = 0\n",
    "        else:\n",
    "            no_improve_count += 1\n",
    "        \n",
    "        if no_improve_count >= patience * 2:\n",
    "            print(f'\\nEarly stopping triggered after {patience * 2} epochs without improvement')\n",
    "            break\n",
    "    \n",
    "    return model\n",
    "def read_data(path, num_cols, cat_cols):\n",
    "    err = 1e-8\n",
    "    id_col = 'isic_id'\n",
    "    return (\n",
    "        pl.from_pandas(path)\n",
    "        .with_columns(\n",
    "            pl.col('age_approx').cast(pl.String).replace('NA', np.nan).cast(pl.Float64),\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.col(pl.Float64).fill_nan(pl.col(pl.Float64).median()),\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.col(cat_cols).cast(pl.Categorical),\n",
    "        )\n",
    "        .to_pandas()\n",
    "        .set_index(id_col)\n",
    "    )\n",
    "\n",
    "def preprocess(df_train, df_test, feature_cols, cat_cols):\n",
    "    \"\"\"\n",
    "    Preprocess the data by encoding categorical variables\n",
    "    \"\"\"\n",
    "    feature_cols = feature_cols.copy()\n",
    "    \n",
    "    encoder = OneHotEncoder(sparse_output=False, dtype=np.int32, handle_unknown='ignore')\n",
    "    encoder.fit(df_train[cat_cols])\n",
    "    print(cat_cols)\n",
    "    new_cat_cols = [f'onehot_{i}' for i in range(len(encoder.get_feature_names_out()))]\n",
    "    \n",
    "    df_train_encoded = encoder.transform(df_train[cat_cols])\n",
    "    df_test_encoded = encoder.transform(df_test[cat_cols])\n",
    "    \n",
    "    for i, col in enumerate(new_cat_cols):\n",
    "        df_train[col] = pd.Categorical(df_train_encoded[:, i])\n",
    "        df_test[col] = pd.Categorical(df_test_encoded[:, i])\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        if col in feature_cols:\n",
    "            feature_cols.remove(col)\n",
    "    feature_cols.extend(new_cat_cols)\n",
    "    \n",
    "    return df_train, df_test, feature_cols, new_cat_cols\n",
    "\n",
    "\n",
    "\n",
    "def cross_validate_model_with_metadata(df_path, feature_cols, device, n_splits=5, base_model_path='swin_metadata_model'):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross validation with metadata support\n",
    "    \"\"\"\n",
    "   \n",
    "    df = pd.read_csv(df_path)\n",
    "    df, val_df, _,stats = split_dataset_with_augmentations_balanced(df, test_size=0.001, val_size=0, random_state=42)\n",
    "    print(stats)\n",
    "\n",
    "\n",
    "    df_old = df.reset_index(drop=True)\n",
    "    cat_cols = [col for col in feature_cols if df[col].dtype == 'object' or df[col].dtype.name == 'category']\n",
    "    num_cols = [col for col in feature_cols if col not in cat_cols]\n",
    "    \n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    metrics = defaultdict(list)\n",
    "\n",
    "    groups = df.groupby('isic_id')['label'].first().reset_index()\n",
    "    unique_ids = groups['isic_id'].values\n",
    "    unique_labels = groups['label'].values\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(unique_ids, unique_labels)):\n",
    "        print(f\"\\nProcessing fold {fold + 1}/{n_splits}\")\n",
    "        \n",
    "        train_ids = unique_ids[train_idx]\n",
    "        val_ids = unique_ids[val_idx]\n",
    "        df = df_old.reset_index()\n",
    "\n",
    "        train_df = df[df['isic_id'].isin(train_ids)].copy()\n",
    "        val_df = df[df['isic_id'].isin(val_ids)].copy()\n",
    "\n",
    "        print(len(train_df[train_df['label'] ==1])/len(train_df))\n",
    "        print(len(val_df[val_df['label'] ==1])/len(val_df))\n",
    "\n",
    "        train_df = read_data(train_df, num_cols, cat_cols)\n",
    "        val_df = read_data(val_df, num_cols, cat_cols)\n",
    "\n",
    "        print(\"Preprocessing data...\")\n",
    "        train_processed, val_processed, updated_feature_cols, new_cat_cols = preprocess(\n",
    "            train_df, val_df, feature_cols, cat_cols\n",
    "        )\n",
    "        print(len(train_processed.columns))\n",
    "        print(train_processed.columns)\n",
    "\n",
    "        train_transform, val_transform = create_transforms(image_size=224)\n",
    "        val_batch_transform = BatchAlbumentation(val_transform)\n",
    "        \n",
    "        train_dataset = PreAugmentedMetadataMelanomaDataset(\n",
    "            csv_file=train_processed,\n",
    "            metadata_cols=updated_feature_cols,\n",
    "            transform=None,\n",
    "            use_augmented=True\n",
    "        )\n",
    "        \n",
    "        val_dataset = PreAugmentedMetadataMelanomaDataset(\n",
    "            csv_file=val_processed,\n",
    "            metadata_cols=updated_feature_cols,\n",
    "            transform=None,\n",
    "            use_augmented=False\n",
    "        )\n",
    "        \n",
    "        train_loader = MetadataAugmentationDataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "            prefetch_factor=3,\n",
    "            persistent_workers=True,\n",
    "            batch_transform=val_batch_transform\n",
    "        )\n",
    "        \n",
    "        val_loader = MetadataAugmentationDataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "            prefetch_factor=3,\n",
    "            persistent_workers=True,\n",
    "            batch_transform=val_batch_transform\n",
    "        )\n",
    "        \n",
    "        num_classes = len(df['label'].unique())\n",
    "        metadata_dim = len(updated_feature_cols)\n",
    "        model = MetaDataFineTuneResnet(num_classes=num_classes, metadata_dim=metadata_dim, fusion_type='attention')\n",
    "\n",
    "        model = model.to(device)\n",
    "        \n",
    "        class_weights = torch.tensor([1.0, 1.0]).to(device)\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.00091, betas=(0.9, 0.999))\n",
    "\n",
    "        model = train_model_with_metadata(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        num_epochs=80,\n",
    "        device=device,\n",
    "        patience=3\n",
    "    )\n",
    "    \n",
    "        fold_model_path = f\"{base_model_path}_fold{fold+1}.pth\"\n",
    "        torch.save(model.state_dict(), fold_model_path)\n",
    "        print(f\"Saved model for fold {fold+1} to {fold_model_path}\")\n",
    "        \n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, metadata, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                metadata = metadata.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(images, metadata)\n",
    "                probs = torch.softmax(outputs, dim=1)[:, 1]\n",
    "                \n",
    "                val_preds.extend(probs.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        fold_auc = roc_auc_score(val_labels, val_preds)\n",
    "        fold_pauc = custom_metric(val_labels, val_preds)\n",
    "        binary_preds = [1 if p >= 0.5 else 0 for p in val_preds]\n",
    "        fold_report = classification_report(val_labels, binary_preds, output_dict=True)\n",
    "        \n",
    "        metrics['auc'].append(fold_auc)\n",
    "        metrics['fold_pauc'].append(fold_pauc)\n",
    "        metrics['precision'].append(fold_report['weighted avg']['precision'])\n",
    "        metrics['recall'].append(fold_report['weighted avg']['recall'])\n",
    "        metrics['f1'].append(fold_report['weighted avg']['f1-score'])\n",
    "        \n",
    "        print(f\"\\nFold {fold+1} Results:\")\n",
    "        print(f\"AUC: {fold_auc:.4f}\")\n",
    "        print(f\"pAUC: {fold_pauc:.4f}\")\n",
    "\n",
    "        print(f\"Precision: {fold_report['weighted avg']['precision']:.4f}\")\n",
    "        print(f\"Recall: {fold_report['weighted avg']['recall']:.4f}\")\n",
    "        print(f\"F1: {fold_report['weighted avg']['f1-score']:.4f}\")\n",
    "    \n",
    "\n",
    "    print(\"\\nAverage Metrics Across All Folds:\")\n",
    "    average_metrics = {\n",
    "        'auc': np.mean(metrics['auc']),\n",
    "        'pauc' : np.mean(metrics['fold_pauc']),\n",
    "        'precision': np.mean(metrics['precision']),\n",
    "        'recall': np.mean(metrics['recall']),\n",
    "        'f1': np.mean(metrics['f1']),\n",
    "        'auc_std': np.std(metrics['auc']),\n",
    "        'pauc_std': np.std(metrics['fold_pauc']),\n",
    "\n",
    "        'precision_std': np.std(metrics['precision']),\n",
    "        'recall_std': np.std(metrics['recall']),\n",
    "        'f1_std': np.std(metrics['f1'])\n",
    "    }\n",
    "    \n",
    "    print(f\"Average AUC: {average_metrics['auc']:.4f} ± {average_metrics['auc_std']:.4f}\")\n",
    "    print(f\"Average pAUC: {average_metrics['pauc']:.4f} ± {average_metrics['pauc_std']:.4f}\")\n",
    "\n",
    "    print(f\"Average Precision: {average_metrics['precision']:.4f} ± {average_metrics['precision_std']:.4f}\")\n",
    "    print(f\"Average Recall: {average_metrics['recall']:.4f} ± {average_metrics['recall_std']:.4f}\")\n",
    "    print(f\"Average F1: {average_metrics['f1']:.4f} ± {average_metrics['f1_std']:.4f}\")\n",
    "    \n",
    "    return metrics, average_metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved test set to CSV\n",
      "{'train': {'size': 6276, 'distribution': {0: 3140, 1: 3136}}, 'val': {'size': 0, 'distribution': {}}, 'test': {'size': 2, 'distribution': {0: 2}}}\n",
      "\n",
      "Processing fold 1/5\n",
      "0.49920255183413076\n",
      "0.5015873015873016\n",
      "Preprocessing data...\n",
      "['sex', 'anatom_site_general', 'tbp_tile_type', 'tbp_lv_location', 'tbp_lv_location_simple', 'attribution']\n",
      "92\n",
      "Index(['index', 'original_name', 'path', 'label', 'is_augmented',\n",
      "       'augmented_name', 'age_approx', 'clin_size_long_diam_mm', 'tbp_lv_A',\n",
      "       'tbp_lv_Aext', 'tbp_lv_B', 'tbp_lv_Bext', 'tbp_lv_C', 'tbp_lv_Cext',\n",
      "       'tbp_lv_H', 'tbp_lv_Hext', 'tbp_lv_L', 'tbp_lv_Lext', 'tbp_lv_areaMM2',\n",
      "       'tbp_lv_area_perim_ratio', 'tbp_lv_color_std_mean', 'tbp_lv_deltaA',\n",
      "       'tbp_lv_deltaB', 'tbp_lv_deltaL', 'tbp_lv_deltaLB',\n",
      "       'tbp_lv_deltaLBnorm', 'tbp_lv_eccentricity', 'tbp_lv_minorAxisMM',\n",
      "       'tbp_lv_nevi_confidence', 'tbp_lv_norm_border', 'tbp_lv_norm_color',\n",
      "       'tbp_lv_perimeterMM', 'tbp_lv_radial_color_std_max', 'tbp_lv_stdL',\n",
      "       'tbp_lv_stdLExt', 'tbp_lv_symm_2axis', 'tbp_lv_symm_2axis_angle',\n",
      "       'tbp_lv_x', 'tbp_lv_y', 'tbp_lv_z', 'sex', 'anatom_site_general',\n",
      "       'tbp_tile_type', 'tbp_lv_location', 'tbp_lv_location_simple',\n",
      "       'attribution', 'onehot_0', 'onehot_1', 'onehot_2', 'onehot_3',\n",
      "       'onehot_4', 'onehot_5', 'onehot_6', 'onehot_7', 'onehot_8', 'onehot_9',\n",
      "       'onehot_10', 'onehot_11', 'onehot_12', 'onehot_13', 'onehot_14',\n",
      "       'onehot_15', 'onehot_16', 'onehot_17', 'onehot_18', 'onehot_19',\n",
      "       'onehot_20', 'onehot_21', 'onehot_22', 'onehot_23', 'onehot_24',\n",
      "       'onehot_25', 'onehot_26', 'onehot_27', 'onehot_28', 'onehot_29',\n",
      "       'onehot_30', 'onehot_31', 'onehot_32', 'onehot_33', 'onehot_34',\n",
      "       'onehot_35', 'onehot_36', 'onehot_37', 'onehot_38', 'onehot_39',\n",
      "       'onehot_40', 'onehot_41', 'onehot_42', 'onehot_43', 'onehot_44',\n",
      "       'onehot_45'],\n",
      "      dtype='object')\n",
      "Number of metadata features: 80\n",
      "Sample metadata columns: ['age_approx', 'clin_size_long_diam_mm', 'tbp_lv_A', 'tbp_lv_Aext', 'tbp_lv_B', 'tbp_lv_Bext', 'tbp_lv_C', 'tbp_lv_Cext', 'tbp_lv_H', 'tbp_lv_Hext', 'tbp_lv_L', 'tbp_lv_Lext', 'tbp_lv_areaMM2', 'tbp_lv_area_perim_ratio', 'tbp_lv_color_std_mean', 'tbp_lv_deltaA', 'tbp_lv_deltaB', 'tbp_lv_deltaL', 'tbp_lv_deltaLB', 'tbp_lv_deltaLBnorm', 'tbp_lv_eccentricity', 'tbp_lv_minorAxisMM', 'tbp_lv_nevi_confidence', 'tbp_lv_norm_border', 'tbp_lv_norm_color', 'tbp_lv_perimeterMM', 'tbp_lv_radial_color_std_max', 'tbp_lv_stdL', 'tbp_lv_stdLExt', 'tbp_lv_symm_2axis', 'tbp_lv_symm_2axis_angle', 'tbp_lv_x', 'tbp_lv_y', 'tbp_lv_z', 'onehot_0', 'onehot_1', 'onehot_2', 'onehot_3', 'onehot_4', 'onehot_5', 'onehot_6', 'onehot_7', 'onehot_8', 'onehot_9', 'onehot_10', 'onehot_11', 'onehot_12', 'onehot_13', 'onehot_14', 'onehot_15', 'onehot_16', 'onehot_17', 'onehot_18', 'onehot_19', 'onehot_20', 'onehot_21', 'onehot_22', 'onehot_23', 'onehot_24', 'onehot_25', 'onehot_26', 'onehot_27', 'onehot_28', 'onehot_29', 'onehot_30', 'onehot_31', 'onehot_32', 'onehot_33', 'onehot_34', 'onehot_35', 'onehot_36', 'onehot_37', 'onehot_38', 'onehot_39', 'onehot_40', 'onehot_41', 'onehot_42', 'onehot_43', 'onehot_44', 'onehot_45']...\n",
      "Number of metadata features: 80\n",
      "Sample metadata columns: ['age_approx', 'clin_size_long_diam_mm', 'tbp_lv_A', 'tbp_lv_Aext', 'tbp_lv_B', 'tbp_lv_Bext', 'tbp_lv_C', 'tbp_lv_Cext', 'tbp_lv_H', 'tbp_lv_Hext', 'tbp_lv_L', 'tbp_lv_Lext', 'tbp_lv_areaMM2', 'tbp_lv_area_perim_ratio', 'tbp_lv_color_std_mean', 'tbp_lv_deltaA', 'tbp_lv_deltaB', 'tbp_lv_deltaL', 'tbp_lv_deltaLB', 'tbp_lv_deltaLBnorm', 'tbp_lv_eccentricity', 'tbp_lv_minorAxisMM', 'tbp_lv_nevi_confidence', 'tbp_lv_norm_border', 'tbp_lv_norm_color', 'tbp_lv_perimeterMM', 'tbp_lv_radial_color_std_max', 'tbp_lv_stdL', 'tbp_lv_stdLExt', 'tbp_lv_symm_2axis', 'tbp_lv_symm_2axis_angle', 'tbp_lv_x', 'tbp_lv_y', 'tbp_lv_z', 'onehot_0', 'onehot_1', 'onehot_2', 'onehot_3', 'onehot_4', 'onehot_5', 'onehot_6', 'onehot_7', 'onehot_8', 'onehot_9', 'onehot_10', 'onehot_11', 'onehot_12', 'onehot_13', 'onehot_14', 'onehot_15', 'onehot_16', 'onehot_17', 'onehot_18', 'onehot_19', 'onehot_20', 'onehot_21', 'onehot_22', 'onehot_23', 'onehot_24', 'onehot_25', 'onehot_26', 'onehot_27', 'onehot_28', 'onehot_29', 'onehot_30', 'onehot_31', 'onehot_32', 'onehot_33', 'onehot_34', 'onehot_35', 'onehot_36', 'onehot_37', 'onehot_38', 'onehot_39', 'onehot_40', 'onehot_41', 'onehot_42', 'onehot_43', 'onehot_44', 'onehot_45']...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davis/miniconda3/envs/cv/lib/python3.8/site-packages/pydantic/main.py:212: UserWarning: blur_limit and sigma_limit minimum value can not be both equal to 0. blur_limit minimum value changed to 3.\n",
      "  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing layer: conv1\n",
      "Freezing layer: bn1\n",
      "Freezing layer: relu\n",
      "Freezing layer: maxpool\n",
      "Freezing layer: layer1\n",
      "Freezing layer: layer2\n",
      "CUDA available: True\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 3060 Ti\n",
      "Model device: cuda:0\n",
      "Testing data pipeline with single batch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/davis/miniconda3/envs/cv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test batch loaded successfully:\n",
      "Inputs shape: torch.Size([64, 3, 224, 224]), device: cpu\n",
      "Metadata shape: torch.Size([64, 80]), device: cpu\n",
      "Labels shape: torch.Size([64]), device: cpu\n",
      "Forward pass successful\n",
      "\n",
      "Starting main training loop...\n",
      "\n",
      "Epoch 1/80\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 79/79 [12:18<00:00,  9.35s/it, loss=0.3158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 0.4061, AUC: 0.8981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 1: 100%|██████████| 20/20 [01:07<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Results:\n",
      "Train Loss: 0.4061 Train AUC: 0.8981\n",
      "Val Loss: 0.3428 Val AUC: 0.9300\n",
      "New best model saved with validation AUC: 0.9300\n",
      "\n",
      "Epoch 2/80\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 79/79 [11:46<00:00,  8.94s/it, loss=0.4967]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Loss: 0.3443, AUC: 0.9270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 2: 100%|██████████| 20/20 [01:06<00:00,  3.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Results:\n",
      "Train Loss: 0.3443 Train AUC: 0.9270\n",
      "Val Loss: 0.3432 Val AUC: 0.9276\n",
      "\n",
      "Epoch 3/80\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:   4%|▍         | 3/79 [00:46<22:18, 17.61s/it, loss=0.2458]"
     ]
    }
   ],
   "source": [
    "\n",
    "num_cols = [\n",
    "    'age_approx',                        \n",
    "    'clin_size_long_diam_mm',            \n",
    "    'tbp_lv_A',                          \n",
    "    'tbp_lv_Aext',                       \n",
    "    'tbp_lv_B',                          \n",
    "    'tbp_lv_Bext',                       \n",
    "    'tbp_lv_C',                          \n",
    "    'tbp_lv_Cext',                       \n",
    "    'tbp_lv_H',                         \n",
    "    'tbp_lv_Hext',                      \n",
    "    'tbp_lv_L',                         \n",
    "    'tbp_lv_Lext',                      \n",
    "    'tbp_lv_areaMM2',                   \n",
    "    'tbp_lv_area_perim_ratio',          \n",
    "    'tbp_lv_color_std_mean',            \n",
    "    'tbp_lv_deltaA',                    \n",
    "    'tbp_lv_deltaB',                    \n",
    "    'tbp_lv_deltaL',                    \n",
    "    'tbp_lv_deltaLB',                   \n",
    "    'tbp_lv_deltaLBnorm',               \n",
    "    'tbp_lv_eccentricity',             \n",
    "    'tbp_lv_minorAxisMM',              \n",
    "    'tbp_lv_nevi_confidence',          \n",
    "    'tbp_lv_norm_border',              \n",
    "    'tbp_lv_norm_color',               \n",
    "    'tbp_lv_perimeterMM',              \n",
    "    'tbp_lv_radial_color_std_max',     \n",
    "    'tbp_lv_stdL',                     \n",
    "    'tbp_lv_stdLExt',                  \n",
    "    'tbp_lv_symm_2axis',             \n",
    "    'tbp_lv_symm_2axis_angle',         \n",
    "    'tbp_lv_x',                        \n",
    "    'tbp_lv_y',                         \n",
    "    'tbp_lv_z',                         \n",
    "]\n",
    "cat_cols = ['sex', 'anatom_site_general', 'tbp_tile_type', 'tbp_lv_location', 'tbp_lv_location_simple', 'attribution']\n",
    "\n",
    "feature_cols = num_cols + cat_cols \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "metrics, average_metrics = cross_validate_model_with_metadata(\n",
    "                                                            df_path='merged_output.csv',\n",
    "                                                            feature_cols=feature_cols,  \n",
    "                                                            device=device,\n",
    "                                                            n_splits=5,\n",
    "                                                            base_model_path='best_model'\n",
    "                                                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
